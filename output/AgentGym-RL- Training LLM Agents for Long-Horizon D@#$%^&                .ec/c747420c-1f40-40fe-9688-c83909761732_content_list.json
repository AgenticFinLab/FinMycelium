[
    {
        "type": "text",
        "text": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning ",
        "text_level": 1,
        "bbox": [
            120,
            127,
            877,
            204
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Zhiheng $\\mathbf { \\times } \\mathbf { i } ^ { 1 * \\dagger }$ ,Jixuan Huang1\\*, Chenyang Liao1\\*, Baodai Huang1, Honglin Guo1, Jiaqi $\\mathsf { L i u ^ { 1 } }$ , Rui Zheng1, Junjie $\\pmb { \\gamma } _ { \\pmb { \\theta } ^ { 1 } }$ , Jiazheng Zhang1, Wenxiang Chen1, Wei $\\mathsf { H e } ^ { 1 }$ , Yiwen Ding1, Guanyu Li1, Zehui Chen², Zhengyin $\\mathsf { D } \\mathsf { u } ^ { 2 }$ ,Xuesong $\\yen 90^ { 2 }$ ,Yufei $\\pmb { \\times } \\pmb { \\ u } ^ { 2 }$ , Jiecao Chen², Tao Guil,3†, Zuxuan $\\boldsymbol { \\mathsf { W } } \\boldsymbol { \\mathsf { U } } ^ { 1 , 3 }$ , Qi Zhang1†, Xuanjing Huang1, Yu-Gang Jiang1 ",
        "bbox": [
            151,
            243,
            844,
            333
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1Fudan University, ByteDance Seed, 3Shanghai Innovation Institute ",
        "bbox": [
            223,
            344,
            772,
            361
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Abstract ",
        "text_level": 1,
        "bbox": [
            455,
            415,
            542,
            433
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch—without relying on supervised fine-tuning (SFT)—across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interaction, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and efectiveness of both the AgentGym-RL framework and ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source complete AgentGym-RL framework—including code and datasets—to empower the research community in developing the next generation of intelligent agents. ",
        "bbox": [
            151,
            443,
            846,
            729
        ],
        "page_idx": 0
    },
    {
        "type": "discarded",
        "text": "Correspondence: zhxi22@m.fudan.edu.cn, tgui@fudan.edu.cn, qz@fudan.edu.cn Code: https://github.com/woooodyy/AgentGym-RL Project: https://AgentGym-RL.github.io ",
        "bbox": [
            153,
            772,
            697,
            816
        ],
        "page_idx": 0
    },
    {
        "type": "discarded",
        "text": "ByteDance Seed ",
        "bbox": [
            112,
            65,
            366,
            88
        ],
        "page_idx": 0
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            22,
            277,
            62,
            723
        ],
        "page_idx": 0
    },
    {
        "type": "discarded",
        "text": "1 ",
        "bbox": [
            493,
            938,
            503,
            949
        ],
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/c2b705e499dc63e3d31a95c81e5b50c7142755a847a0a7c30143ae7e485d6745.jpg",
        "image_caption": [
            "Fur Let:Peroranc propretay models, pen-sourmodes, and urRL moels acoss differet gnc tasks Rihean mode salWorkin, urrk nmet sustnt - . "
        ],
        "image_footnote": [],
        "bbox": [
            119,
            97,
            870,
            352
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "1 Introduction ",
        "text_level": 1,
        "bbox": [
            114,
            420,
            259,
            438
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "As Large Language Models (LLMs) have largely developed [2, 11, 41, 65, 84], their applications have extended from chatbots to autonomous agents that can handle long-horizon real-world tasks [39, 79]. Given a complex task, these agents interact with the environment, making a series of intelligent decisions to achieve the goal [95]. Analogous to human cognitive development, LLM agents are expected to acquire new knowledge and skills by actively exploring and interacting with the environment [42, 77]. Therefore, a natural approach is to train these agents using Reinforcement Learning (RL) [62]. ",
        "bbox": [
            114,
            449,
            885,
            541
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Despite the progress of RL in areas like LLM reasoning [12, 18, 23, 66, 68, 76], most existing studies are restricted to single-tur tasks, where models are not required toengage in multi-turninteraction with complex environments [71]. While some recent efforts have attempted to extend RL to train LLM agents with multi-urn capabilities [3, 26, 30, 47, 71, 95], these works are limited in task complexity and environment diversity. Furthermore, they struggle with optimization stability and eficiency, resulting in suboptimal performance. Criticay, the community currently lacks a unife, end-to-end interactive multi-turn RL framework that is proven to be effective across a wide range of real-world scenarios and environments for training LLM agents without SFT as a preliminary step [12]. ",
        "bbox": [
            112,
            547,
            885,
            670
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "To bridge this gap, we introduce AgentGym-RL, a new framework for training LLM agents for multi-turn interactive decision-making through RL (Figure 2). Designed with a modular and decoupled architecture, AgentGym-RL enables clean separation of agents, environments, and learning algorithms—offering high extensibility and fexibility for diverse research needs. The framework supports mainstream RL algorithms, including PPO [53], GRPO [54], and REINFORCE++ [21], and is equipped with a wide range of real-world scenarios, e.g., web navigation [13, 87, 94], deep search [26, 72], digital games [15, 45], embodied tasks [7, 58], and scientific tasks [60, 69]. ",
        "bbox": [
            112,
            675,
            885,
            782
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Furthermore, to tackle the explorationexploitation trade-of and improve optimization stability in agent RL training, we propose ScalingInter-RL, a method that progressively extends the agentenvironment interaction horizon during training. The core insight of this approach is to let the agent adapt to the environment in stages: beginning with exploitation to achieve reliable mastery of basic skill and simple tasks; subsequently increasing interaction horizon to promote exploration, refine behaviors, overcome shortcuts, and adress more complex challenges. This progressive interaction-scaling strategy enables the agent to uncover richer interaction patterns (e.g., planning and reflection) and cultivate a broader set of skills and behaviors over time. ",
        "bbox": [
            112,
            789,
            883,
            910
        ],
        "page_idx": 1
    },
    {
        "type": "discarded",
        "text": "2 ",
        "bbox": [
            493,
            938,
            504,
            949
        ],
        "page_idx": 1
    },
    {
        "type": "image",
        "img_path": "",
        "image_caption": [
            ""
        ],
        "image_footnote": [],
        "page_idx": 2
    },
    {
        "type": "image",
        "img_path": "images/1085d4c22d46045bac467bad66081bdace0df10adbe66cbeb01972cbc91c77cf.jpg",
        "image_caption": [
            "FigureOverviw f the gentGym-RL ramework. It atures a decuple fexible,andextensible rchiture, ce het, e e no. Ito ve , environments, and algorithms. "
        ],
        "image_footnote": [],
        "bbox": [
            127,
            128,
            880,
            429
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Our extensive experiments prove that AgentGym-RL delivers consistent and significant performance gains for agents across five tasks spanning 5 scenarios (Figure 1a). Open-source models (e.g., Qwen-2.5-7B [83]) trained with our framework and method achieved an average improvement of 33.65 points, matching—or even outperforming—larger commercial, closed-source models such as OpenAI-03 [42] and Gemini-2.5-Pro [10]. We also conducted numerous analytical experiments to provide key findings and insights, showing that scaling post-training and test-time compute have significant potential for developing agentic intelligence (Figure 1b). We hope our work will be a valuable contribution to the community's progress. ",
        "bbox": [
            112,
            508,
            885,
            614
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "In summary, our main contributions are: ",
        "bbox": [
            114,
            622,
            405,
            637
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "1. We propose and open-source AgentGym-RL, a new unified, modular, and fexible end-to-end RL framework designed for agent multi-turn interactive decision-making that includes a diversity of scenarios and environments.   \n2. We propose ScalingInter-RL, a progressive interaction-scaling framework that incrementally adapts agents to their environment, facilitating the refinement of interaction patterns and skill acquisition. It enhances optimization stability in RL and achieves a balance between exploration and exploitation.   \n3. Our extensive experiments demonstrate that AgentGym-RL and ScalingInter-RL deliver significant and consistent performance gains, matching or exceeding commercial models. In addition, we conduct empirical analyses that yield critical insights into agent design and operational paradigms, offering valuable guidance and resources for future research. ",
        "bbox": [
            127,
            643,
            887,
            810
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2 Preliminaries ",
        "text_level": 1,
        "bbox": [
            114,
            827,
            272,
            844
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2.1 Formulation ",
        "text_level": 1,
        "bbox": [
            114,
            856,
            263,
            872
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "In this work we study the multiturnteracive deci-makg tasks iagenictasksand we moelhem as a Partially Observable Markov Decision Process (POMDP) $( \\mathcal { U } , \\mathcal { S } , \\mathcal { A } , \\mathcal { O } , \\mathcal { T } , r )$ like Xi et al. [77], Zhou et al. ",
        "bbox": [
            112,
            880,
            885,
            911
        ],
        "page_idx": 2
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            130,
            101,
            155,
            119
        ],
        "page_idx": 2
    },
    {
        "type": "discarded",
        "text": "3 ",
        "bbox": [
            493,
            938,
            504,
            949
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "[95], where $\\mathcal { U } , S , \\mathcal { O } , T : S \\times \\mathcal { A }  S , r : \\mathcal { U } \\times S  \\mathbb { R }$ represents the instruction space, the state space, the action spa theobservation space the deterministi statetransition fnction and therewarunction, respecively. ",
        "bbox": [
            112,
            98,
            885,
            128
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Given a task instruction $u \\in \\mathcal { U }$ , the agentic task requires the LLM agent to generate a sequence of actions $a _ { k } ^ { \\scriptscriptstyle T } \\sim \\pi _ { \\theta } ( \\cdot | s _ { k } )$ based on its policy $\\pi _ { \\theta }$ parameterized by $\\theta$ to complete the given task, where $a _ { k } \\in { \\mathcal { A } }$ , and $s _ { k } \\in S$ and $T$ is the thinking path [88]. The agent then receives an observation $o _ { k } \\in \\mathcal { O }$ from the environment, and the state is then transitioned to ${ \\mathcal { T } } ( s _ { k } , a _ { k } ) = s _ { k + 1 }$ . Finally after $N$ turns of interactions, the environment $e$ provides an outcome reward $r ( \\tau ) \\in [ 0 , 1 ]$ to describe the completion of the multi-turn interactive decision-making tasks. ",
        "bbox": [
            112,
            135,
            885,
            213
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "2.2 Policy Gradient ",
        "text_level": 1,
        "bbox": [
            114,
            227,
            295,
            244
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "We utilize policy gradient methods [63] that optimizes our policy to maximize the expected cumulative reward. Unlike value-based methods that estimate the value function to derive a policy, policy gradient methods directly search the policy parameter space to find the optimal policy. ",
        "bbox": [
            112,
            251,
            885,
            297
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "The core idea of policy gradient methods is to perform gradient ascent according to the objective $J ( \\theta )$ , which is a function of the policy parameters $\\theta$ . Specifically, $J ( \\theta )$ represents the expected cumulative reward the agent anticipates receiving when following policy $\\pi _ { \\theta }$ and interacting with the environment. Mathematically, this is expressed as the expectation of the total reward $r ( \\tau )$ over trajectories $\\tau$ generated by the policy: ",
        "bbox": [
            112,
            304,
            885,
            366
        ],
        "page_idx": 3
    },
    {
        "type": "equation",
        "img_path": "images/4b25037ee76d02d459fb3b59dbf19ef2bddebca7858709caafeac69e7c15a3cd.jpg",
        "text": "$$\nJ ( \\theta ) = \\mathbb { E } _ { \\tau \\sim \\pi _ { \\theta } } \\left[ r ( \\tau ) \\right]\n$$",
        "text_format": "latex",
        "bbox": [
            429,
            387,
            568,
            404
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "To perform gradient ascent on $J ( \\theta )$ , we require the policy gradient $\\nabla _ { \\boldsymbol { \\theta } } J ( \\boldsymbol { \\theta } )$ . In the vanilla policy gradient methods, the policy gradient can be estimated by: ",
        "bbox": [
            112,
            417,
            890,
            449
        ],
        "page_idx": 3
    },
    {
        "type": "equation",
        "img_path": "images/79d1d8d56728073112dfd5ebdd740b4124317f73569fdf140b01f120d6246b4c.jpg",
        "text": "$$\n\\nabla _ { \\theta } J ( \\theta ) = \\mathbb { E } _ { \\tau \\sim \\pi _ { \\theta } } \\left[ r ( \\tau ) \\sum _ { k = 0 } ^ { K } \\nabla _ { \\theta } \\log \\pi _ { \\theta } ( a _ { k } | s _ { k } ) \\right]\n$$",
        "text_format": "latex",
        "bbox": [
            343,
            467,
            655,
            508
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "where $\\pi \\theta$ is the policy parameterized by $\\theta$ , $\\tau$ represents a trajectory consisting of a sequence of states and actions, $a _ { k }$ and $s _ { k }$ are the action and state at time step $k$ , and $r ( \\tau )$ is the reward of the trajectory $\\tau$ . ",
        "bbox": [
            112,
            521,
            892,
            553
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "With the policy gradient estimated, we can optimize the parameters $\\theta$ of the policy $\\pi _ { \\theta }$ towards a direction of maximizing the expected cumulative reward with the gradient descent method by: ",
        "bbox": [
            112,
            559,
            888,
            590
        ],
        "page_idx": 3
    },
    {
        "type": "equation",
        "img_path": "images/709899f8810f29d24a2d457a100381b3fe3b903e6130b8f190d25b0364abf043.jpg",
        "text": "$$\n\\theta _ { n e w } = \\theta _ { o l d } - \\alpha \\nabla _ { \\theta } J ( \\theta )\n$$",
        "text_format": "latex",
        "bbox": [
            416,
            612,
            581,
            628
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "where $\\alpha \\in [ 0 , 1 ]$ is the learning rate. Mainstream RL algorithms for training LLMs include PPO [53], GRPO [54], REINFORCE++ [21], and RLOO [30]—all of which are integrated into our framework. ",
        "bbox": [
            111,
            641,
            887,
            674
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "3 The AgentGym-RL Framework ",
        "text_level": 1,
        "bbox": [
            112,
            688,
            442,
            707
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "3.1 Architecture Overview ",
        "text_level": 1,
        "bbox": [
            114,
            717,
            354,
            733
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "The AgentGym-RL framework is built on AgentGym [77], which provides several basic interactive environments for LLM agents. Our main extensions focus on three aspects: ",
        "bbox": [
            112,
            741,
            885,
            772
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "1. Introducing more realistic environments and tasks (e.g. Deep Search tasks) to facilitate the development of more general agents.   \n2. Incorporating a diverse set of online reinforcement learning algorithms covering both classical and state-of-the-art methods, to ensure consistency with current research frontiers, and offer an extensible foundation for the community to build upon.   \n3. Implementing extensive engineering optimizations and agent-environment co-design, such as improved rollout parallelization and memory-leak mitigation. ",
        "bbox": [
            130,
            779,
            885,
            901
        ],
        "page_idx": 3
    },
    {
        "type": "discarded",
        "text": "4 ",
        "bbox": [
            493,
            938,
            504,
            949
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "# Stage 1: Generate responses # Stage 2: Prepare experience batch $=$ gen_batch_from_rollout_handlers(handlers) batch $=$ actor.compute_log_prob(batch) batch $=$ reference.compute_ref_log_prob(batch) batch $=$ compute_advantages(batch, method=\"grpo\") ",
        "bbox": [
            130,
            108,
            504,
            141
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            539,
            108,
            861,
            164
        ],
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/dbb95960b9959356be13a7b57e94668229fd4d5ba86b08ed459b511bdbf62634.jpg",
        "image_caption": [
            "Figur 3Pseuocode demnstrating the exampl usage  our proposframework (provied APIs marke range), alongside siplietheorticaldiagramllustratihe agent evinmentnteraction and traiig ppene "
        ],
        "image_footnote": [],
        "bbox": [
            132,
            117,
            501,
            406
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "# Stage 3: Actor training actor.update_actor(batch) ",
        "bbox": [
            540,
            174,
            705,
            196
        ],
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/354f243c9522966ea6c61d7274e7178ca921a140833677d5a032003e22122daf.jpg",
        "image_caption": [],
        "image_footnote": [],
        "bbox": [
            545,
            218,
            834,
            404
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "As shown in Figure 2, the framework is organized into three main components: ",
        "bbox": [
            114,
            481,
            678,
            496
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "The Environment module provides diverse scenarios via a standardized server-client architecture with unified HTTP protocols.   \nThe Agent module encapsulates the reasoning and decision-making process of agents in multi-turn interactions, with support for advanced mechanisms such as long-horizon planning and self-reflection.   \nThe Training module implements reinforcement learning pipelines and other training methods to optimize agent policies. ",
        "bbox": [
            137,
            502,
            885,
            609
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "A more detailed description of our architecture is shown in Appendix A. ",
        "bbox": [
            116,
            616,
            632,
            632
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Given a batch of user queries and initial environment states, our framework initializes multiple independent evironment cients in parallelEach client interacts exclusively with a singe agent, ensurin that execuions are isolated and non-interfering. In every client, the agent generates an action that is executed in the environment, which then returns the updated state and reward for the next decision. A batch of such trajectoriesis collected concurrentlyacross clients and subsequently fed into the trainin module to update the agent policy. The overall workfow and corresponding pseudocode of our framework are shown in Figure 3. ",
        "bbox": [
            114,
            638,
            883,
            729
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "3.2 Features and Characteristics ",
        "text_level": 1,
        "bbox": [
            114,
            743,
            418,
            760
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "In this section, we highlight the key features of the AgentGym-RL framework, covering four aspects: environment coverage, algorithm support, architectural advantages, and open-source contributions. ",
        "bbox": [
            112,
            767,
            885,
            797
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "3.2.1 Diverse scenarios and environments. ",
        "text_level": 1,
        "bbox": [
            112,
            814,
            460,
            830
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "To build LLM agents capable of multi-turn sequential decision-making for complex tasks in real-world environments, AgentGym-RL covers a broad spectrum of scenarios to comprehensively evaluate and foster the agent's ability to perceive its environment, long-term planning towards a goal, in-depth reasoning for making intelligent decisions, aptitude for reflection and correction when facing setbacks or making mistakes. ",
        "bbox": [
            114,
            838,
            883,
            900
        ],
        "page_idx": 4
    },
    {
        "type": "discarded",
        "text": "5 ",
        "bbox": [
            493,
            938,
            504,
            949
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "It includes: ",
        "bbox": [
            114,
            99,
            194,
            113
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Web Navigation: Interacting with dynamic websites for tasks such as booking fights or extracting structured information, which requires agents to follow instructions, interpret textual and visual content, manipulate dynamic interfaces, and plan multi-step actions.   \nDeep Search: Performing multi-step, goal-directed queries with tools like browsers or Python interpreters, demanding strong information-seeking, multi-hop reasoning, long-term memory, and knowledge synthesis across sources.   \nDigital Games: Exploring and solving problems in interactive game-like environments, emphasizing real-time decision-making, strategy development, and adaptability to complex, dynamic setings.   \nEmbodied Tasks:Controlling virtual or physical bodies for navigation, manipulation, and task execution, which calls for goal-directed planning, spatial reasoning, and robust perception-action grounding.   \n•Scientific Tasks: Conducting experiments and solving problems in physically grounded, knowledgeintensive settings, requiring precise execution, dynamic interpretation of feedback, evidence-based reasoning, and iterative hypothesis refinement. ",
        "bbox": [
            133,
            121,
            887,
            348
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "3.2.2 Comprehensive RL algorithm support. ",
        "text_level": 1,
        "bbox": [
            114,
            364,
            472,
            381
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "While the original AgentGym supports only a limited se of training methods based on supervised fine-tuning, AgentGym-RL places online reinforcement learning at its core, empowering agents to dynamically explore and adapt through continuous interactions with the environment. ",
        "bbox": [
            114,
            388,
            885,
            433
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "AgentGym-RL implements a suite of mainstream online RL algorithms: (1) PPO[53], a policy gradient method that improves training stability by clipping policy updates to prevent overly large steps, simpliying the trust-region concept from TRPO[52] while maintaining strong empirical performance; (2) GRPO[54], a PPO-derived method that normalizes rewards within groups of sampled actions per state and applies PPO-style clipping, reinforcing higher-performing actions relative to others; (3) RLO0[30], a REINFORCE variant that uses the average reward of other samples in the same batch as a per-sample baseline, reducing variance in policy gradient estimates; (4) REINFORCE $^ { + + }$ [21], an enhanced REINFORCE[73] algorithm that integrates PPO-style clipping and KL penalties, enabling more stable, simpler, and computationally efficient training without the need for a critic network. ",
        "bbox": [
            112,
            440,
            883,
            577
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Beyond online RL, the framework also supports a broad range of complementary training paradigms: SFT (Supervised Fine-Tuning)[44] is a standard training method where the agent learns to imitate expert demonstrations or golden trajectories step by step. DPo (Direct Preference Optimization)[50] is a variant of reinforcement learning that does not involve online interaction with the environment; instead, it learns from pre-collected preference pairs. For rejection sampling[35], we support methods like AgentEvol[77], which iteratively fine-tunes agents on trajectories generated by themselves and fltered based on task success. ",
        "bbox": [
            112,
            584,
            883,
            675
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "3.2.3 Extensibility, Scalability, and Reliability. ",
        "text_level": 1,
        "bbox": [
            114,
            693,
            493,
            708
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Since AgentGym-RL is primarily designed to support large-scale reinforcement learning research and development for the community, we have carried out extensive engineering design, practice, and optimization to ensure the framework's extensibility, scalability, and reliability. ",
        "bbox": [
            114,
            715,
            883,
            761
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Extensibility. Extensibility is critical for supporting evolving research needs, allowing a framework to accommodate new environments, agent architectures, and training strategies without disrupting existing components. In our system, we adopt a modular and decoupled design, where the core components—Environment, Agent, and Training—are fully plug-and-play. As a result, researchers can easily incorporate new reinforcement learning objectives, reward functions, or sampling techniques, facilitating reproducible experiments and enabling exploration across a wide spectrum of algorithmic directions. For example, a new environment can be introduced by simply inheriting from BaseEnvClient and implementing the required methods such as reset(), step(), and observe(). Once implemented, the new environment can be seamlessly used with existing agent architectures and training routines, enabling rapid experimentation without modifying any of the core framework components. ",
        "bbox": [
            138,
            768,
            885,
            890
        ],
        "page_idx": 5
    },
    {
        "type": "discarded",
        "text": "6 ",
        "bbox": [
            493,
            938,
            504,
            949
        ],
        "page_idx": 5
    },
    {
        "type": "image",
        "img_path": "images/6d0cbe111a922236fd0078dd4b5e0c40337bf34c3ac6f1801d5001058390ff50.jpg",
        "image_caption": [
            "Figure 4 An overview of the visualized user interface of our framework. "
        ],
        "image_footnote": [],
        "bbox": [
            120,
            106,
            869,
            262
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            147,
            324,
            885,
            354
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Scalability. Recent advances in reinforcement learning increasingly rely on large-scale training, involving massive amounts of data and extended interaction sequences, which poses significant challenges for system scalability. To meet these challenges, a framework must be able to scale both in parallelism and interaction duration. We implemented a series of optimizations to achieve this. For example, we replaced WebArena's default single-browser-per-process design with a subprocess-based architecture, enabling a single server to manage multiple Chromium instances concurrently and thereby enhancing parallism. Similarly, in SciWorld environment, we redesigned the environment's initialization and reset routines tosupport robust parallel creation and resetting of multiple instances, resolving previous failures in concurrent instantiation. In addition, we support longer training horizons through a full-reset interface in WebArena, which restores each web server to its initial state after every episode and mitigates state inconsistencies over time. Together, these optimizations allow our framework to scale effectively, facilitating large-scale training and enabling the research community to conduct a broad range of experiments. ",
        "bbox": [
            137,
            363,
            883,
            558
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Reliability. Large scale multi-turn agent RL training poses a significant challenge to system reliability, that is, the ability to maintain consistent and reliable operation over long training periods. To achieve reliabilty, framework must prevent filures that couldisrupt training andensure that criticl resourcs are managed correctly. For instance, we optimized the memory management implementation in TextCraft. The original environment suffered from a memory leak in its recursive crafting_tree implementation, where redundant self-replication of a list structure caused exponential memory growth and eventual crashes during training. We resolved this issue by refactoring the recursion to eliminate redundant copies. Likewise, in SciWorld, a memory leak in its internal clock mechanism caused progressive memory accumulation and instability during extended rollouts. We addressed this issue by refactoring the clock implementation to eliminate the leakage. Through our optimization, the framework provides a reliable environment for long-horizon training, ensuring consistent and uninterrupted operation over extended interaction sequences. ",
        "bbox": [
            140,
            565,
            885,
            746
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Collectively, these design and optimizations remove major engineering bottlenecks and make reproducible, large-scale RL experiments feasible across heterogeneous environments. ",
        "bbox": [
            116,
            755,
            883,
            785
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "3.2.4 Open-source availability and community extensibility. ",
        "text_level": 1,
        "bbox": [
            114,
            801,
            606,
            818
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "We design AgentGym-RL to foster a collaborative ecosystem where community contributions directly accelerate methodological progress while upholding verifiable research standards. AgentGym-RL is released as an opensource framework under permissive licensing, built upon established open-source frameworks such as veRL[56] and AgentGym[77] while maintaining full open-source availability. The framework provides comprehensive documentation, reproducible training pipelines, and standardized APIs to ensure research transparency and practical adoption. Its modular architecture-which includes clearly defined extension points-enables the seamless integration of new environments and training methods, allowing the research community to extend functionality without disrupting the core workflows. To facilitate probing of data and model behaviors, we provide an interactive user interface, which streamlines empirical analysis for iterative development. ",
        "bbox": [
            112,
            825,
            883,
            901
        ],
        "page_idx": 6
    },
    {
        "type": "discarded",
        "text": "7 ",
        "bbox": [
            493,
            938,
            504,
            948
        ],
        "page_idx": 6
    },
    {
        "type": "image",
        "img_path": "images/763d3811cc239dd8219fe196324471abdccfcef62beae3f47a13639c5b0d5a81.jpg",
        "image_caption": [
            "Fu heSagnR. hee apt al ua sk par e stronger agent. "
        ],
        "image_footnote": [],
        "bbox": [
            120,
            98,
            872,
            457
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            112,
            551,
            883,
            612
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Usability, reproducibility and standardized evaluation. AgentGym-RL is designed to be user-friendly for the community. To systematically address reproducibility challenges in LLM-based reinforcement learning, AgentGym-RL institutes a standardized evaluation process and reproducible training pipelines. This design enforces uniform metrics and consistent experimental procedures to ensure fair comparisons. We provide easy-to-setup reproduction scripts that automate the entire workfow, from environment configuration to final evaluation. This desigenables researchers to replicate priorfndings with high fidelity and significanly lowers the barrier for building upon existing work, thereby promoting verifiable research standards. ",
        "bbox": [
            114,
            628,
            883,
            736
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Visualized user interface for observability and analysis. As shown in Figure 4, AgentGym-RL includes an interactive user interface designed tofacilitate the probing o data and model behaviors.This tool streamlines empirical analysis by enabling researchers to perform a fine-grained, step-by-step inspectionof an agent's decision-making process. It allows for the replay and examination of full interaction trajectories, visualizin the interplay betwee environmentalbservations, the agent's internal reasoning, and its resutin acons.This capability proviesdirecinsights intomodel perorance and failure mode, therbyacceeratin the iterative development and debugging cycle. ",
        "bbox": [
            112,
            752,
            885,
            859
        ],
        "page_idx": 7
    },
    {
        "type": "discarded",
        "text": "8 ",
        "bbox": [
            493,
            938,
            504,
            948
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "3.3 ScalingInter-RL: Progressive Scaling Interaction for Agent RL ",
        "text_level": 1,
        "bbox": [
            111,
            97,
            699,
            114
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Motivation and core insight. When assigned a task, an agent engages in iterative interactions with the environment—observing changes, reasoning about them, and executing subsequent actions. Through this cycle, the agent explores and experiments thoroughly, ultimately reaching the target state. This process is analogous to inference-compute scaling in LLM reasoning (as exemplified by OpenAI o1 and DeepSeek-R1), where additional computational resources are allocated at test time or during RL rollouts, allowing the model to reason more deeply before producing a final answer. ",
        "bbox": [
            112,
            121,
            883,
            213
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "In comparison, we argue that beyond relying on internal reasoning to select the next action, agents should also expand their external interactions with the environment to ensure suffcient exploration and accumulate richer context toward the final goal—capturing a form of practice-driven insight. Yet, our preliminary experiments indicate that beginning with a large number of interaction turns often leads the model into redundant reasoning andunprductiveactions, ultimately causingtraining collapse and degraded performance. Conversely, constraining the number of interactions to remain consistently small tends to narrow exploration and limits the agent's ability to master diverse patterns. This motivates us to propose our method. ",
        "bbox": [
            112,
            219,
            883,
            325
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Method. We draw inspiration from reinforcement learning for LLM reasoning [12, 18, 23, 66, 68, 76] and propose ScalingInter-RL, a training approach designed to balance exploration and exploitation while ensuring stableoptimizationAt its cor isa progressiv horizon-scalig strategy that adaptively djusts he nub interaction turns during RL. The bjective is to maximize the expected terminal reward under a constrained interaction budget: ",
        "bbox": [
            112,
            342,
            885,
            417
        ],
        "page_idx": 8
    },
    {
        "type": "equation",
        "img_path": "images/3a8c34369269143138d9e5786a7ec0aab5b73616ef06f77a9d2478e3d8729323.jpg",
        "text": "$$\nJ ( \\theta ) = \\mathbb { E } _ { \\tau \\sim \\pi _ { \\theta } } \\left[ r \\left( \\tau \\right) \\right] ,\n$$",
        "text_format": "latex",
        "bbox": [
            424,
            420,
            571,
            434
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "where each trajectory $\\tau = \\left( a _ { 0 } ^ { T } , o _ { 1 } , a _ { 1 } ^ { T } , \\dots , a _ { K - 1 } ^ { T } , o _ { K } \\right)$ is sampled from the current policy $\\pi \\theta$ , with $K$ representing the total number of interaction turns. To enable the agent to rapidly learn effective behaviors under lmited interactinresoures, we begntrainng wit a sma horizon.By nitly cnstrainin the horizon, thent learns to exploit its policy with maximum efficiency, achieving early proficiency on simple tasks, and laying the groundwork for deeper, long-horizon reasoning. As training progresses, we introduce a monotonic schedule $\\{ h _ { 1 } < h _ { 2 } < \\cdots < h _ { n } \\}$ , where $h _ { t }$ defines the maximum number of interaction turns allowed during phase t: ",
        "bbox": [
            112,
            446,
            885,
            540
        ],
        "page_idx": 8
    },
    {
        "type": "equation",
        "img_path": "images/be5804d2f71c3627bb7ca0df835f0377f4919fc423a2cacf342821befa1e3dcd.jpg",
        "text": "$$\n\\tau _ { t } \\sim \\pi _ { \\theta } \\left( \\tau \\mid h _ { t } \\right) , \\mathrm { s u b j e c t ~ t o } \\ K _ { t } \\le h _ { t } .\n$$",
        "text_format": "latex",
        "bbox": [
            364,
            564,
            632,
            578
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "The horizon $h _ { t }$ is updated every $\\Delta$ training steps according to a curriculum schedule: ",
        "bbox": [
            107,
            592,
            725,
            608
        ],
        "page_idx": 8
    },
    {
        "type": "equation",
        "img_path": "images/7f84c287ae9555c45fa634a991ebce68be438cbd3b599d7951a2b6f86ddeda1c.jpg",
        "text": "$$\nh _ { t + 1 } = h _ { t } + \\delta _ { h } ,\n$$",
        "text_format": "latex",
        "bbox": [
            444,
            632,
            552,
            645
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "where $\\delta _ { h }$ is an adaptive increment. As the horizon increases, the agent is incentivized to explore longer decision paths, facilitating the emergence of higher-order cognitive behaviors such as planning, reflection, and strategic backtracking, which is similar to the length-scaling phenomenon in RLVR for large reasoning models [3, 12, 37]. This phased scaling allows ScalingInter-RL to align the depth of interaction with the agent'evolving polic capabilities, bridgin efficient early-stage exploitation and long-horizon generalization. ",
        "bbox": [
            112,
            660,
            885,
            736
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "4 Experiments ",
        "text_level": 1,
        "bbox": [
            114,
            752,
            269,
            770
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "To verify the stability and effectiveness of the AgentGym-RL framework, we conduct extensive experiments across a diverse set of scenarios and environments. Our results demonstrate that LLM agents are capable of exploring and learning from scratch based solely on environment feedback, without the need for prior supervised fine-tuning, ultimately achieving performance that is comparable to, or even surpasses, that of commercial closed-source models such as OpenAI o3. ",
        "bbox": [
            112,
            780,
            885,
            858
        ],
        "page_idx": 8
    },
    {
        "type": "discarded",
        "text": "9 ",
        "bbox": [
            493,
            938,
            504,
            948
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "4.1 Experimental Settings ",
        "text_level": 1,
        "bbox": [
            114,
            97,
            354,
            114
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Scenarios, Environments and Tasks. As mentioned before, we include five scenarios in AgentGymRL. Specifically, for web navigation, we include WebArena [94] which is a realistic and reproducible web environment containing four distinct domains prevalent on the internet: online shopping, discussion forums, collaborative development, and business content management; for deep search, we include a RAG-based environment [19, 26, 28, 32, 38, 46, 67, 85] which enables LLMs to interact with search engines and solve multi-turn retrieval and reasoning tasks; for digital games, we include TextCraft [45], a text-based crafting game environment in which agents complete tasks via natural language interactions and task-based planning; for embodied tasks, we include BabyAI [7] which provides a controllable grid world with text instructions for embodied reasoning in simulated environments; for scientific tasks, we include SciWorld [69] which ffers a scientific exploration simulator where agents conduct scientific experiments through text-driven reasoning cycles. ",
        "bbox": [
            112,
            121,
            885,
            287
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Baselines and backbone models. We leverage Qwen-2.5-3B and Qwen-2.5-7B [83] as our primary backbone models. We introduce the closed-source Gemini 2.5 Pro [10], OpenAI 03 [42], and GPT-4o [22] as baselines. Additionally, we include the open-source DeepSeek-R1 [12], Qwen-2.5-72B [83], Llama-3.1-8B [14], and Llama-3.1-70B [14] models for comparison. ",
        "bbox": [
            112,
            304,
            885,
            366
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Detailed settings of each environment. We provide detailed descriptions of the tools, APIs, and experimental settings for each environment in Appendix B. ",
        "bbox": [
            111,
            382,
            885,
            412
        ],
        "page_idx": 9
    },
    {
        "type": "image",
        "img_path": "images/409880cccf636e98ee5c6db94e5584de274cd8390f136fd8fb0d086ce9c076d2.jpg",
        "image_caption": [
            "Figure 6 Training rewards in different environments. "
        ],
        "image_footnote": [],
        "bbox": [
            117,
            425,
            877,
            762
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "4.2 Overall Results, Findings, and Insights ",
        "text_level": 1,
        "bbox": [
            112,
            810,
            498,
            828
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "The main results are shown in Figure 1, Table 1, Table 2, Table 3, Table 4, and Table 5. In this section, we discuss the overall findings and insights. ",
        "bbox": [
            112,
            835,
            882,
            866
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Reinforcement learning generally improves agentic intelligence of open-source LLMs to the level of proprietary models. As illustrated in Figure 1, the AgentGym-RL-7B model not only outperforms other open-source models by a large margin but also demonstrates a clear lead in average success rate over leading closed-source models like GPT-4o and Gemini-2.5-Pro across five different scenarios. This achievement hiligts urfmework's effectivenesinabling models tolear anmakecisions incmplenteciv tasks, successfully bridging the performance gap between open-source and proprietary models on advanced intelligent assignments. ",
        "bbox": [
            111,
            882,
            883,
            914
        ],
        "page_idx": 9
    },
    {
        "type": "discarded",
        "text": "10 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            112,
            98,
            883,
            174
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "ScalingInter-RL boosts performance significantly and consistently. Instead of relying on extensive hyperparametertuning, we et the transition points between phass according to the totaloptimization stepso the original RL process. As shown in our results, ScalingInter-RL consistently outperforms the baseline across diverse environments and tasks. Notably, it delivers more than a $1 0 \\%$ improvement on WebArena, bringing performance close to that of closed-source commercial models. On the TextCraft benchmark, it surpasses the base model by 30 points, achieving state-of-the-art results. These findings highlight the effectiveness o our apprach in strikina balance between exploration and exploitation i reiorcement learningAs illustrated in Figure 6, experiments across different environments show that leveraging our AgentGym-RL framework with the ScalingInter-RL algorithm yields stable, sustained, and substantial reward improvements. ",
        "bbox": [
            112,
            190,
            883,
            327
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Large interaction budget accelerates early gains but ultimately leads to unstable training. As shown in Figure 7, we observe that using a larger maximum interaction turn (e.g., 10) achieves higher permance n theary ta compare to horter-tur setting (g, but rapidly colapses astri progresses.This indicates that excessiv exploration n arl stages trainig is not necessarily a god Before establishing a solid foundation, the agent may perform unproductive and ineffiient exploration, leading to the risk of training instability. By contrast, shorter rounds restrict early exploration but provide more stable learning signals, leading to more reliable long-term performance. Taken together, these contrasting dynamics between longer and shorter turns motivate our ScalingInter-RL method, which progressively extends the interaction horizon during training. ",
        "bbox": [
            112,
            344,
            885,
            481
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "ScalingInter-RL demonstrates more stable and efficient training dynamics during RL optimization.As shown in Figure 7, our method is nitilly constrained by the number o interaction turns. Althoug it strugglesto fully master dficult tasks at rst, by exploitifoudatial skills and kowl t achieves a noticeable increase in rewards; later, as it engages in more turns of interaction and exploration with the environment, it shapes its reasoning paradigm and interaction behaviors, ultimately reaching a high level of performance. In contrast, RL with fewer turns yields diminishing returns in later stages and hits a perormance cilin, whileRL with large interaction budget quickly collapses.Furthermore, just asis served with RL for reasoning models [3], our gradual scaling of interactions dramatically reduces the computational resources and time required in RL phase, enabling more efficient optimization. ",
        "bbox": [
            112,
            498,
            883,
            635
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Post-training and test-time compute show higher scaling potential than model size. A key insight from our experiments is that strategic investment in post-training and test-time computation is more impactful than merely increasing a model's parameter count. Figure 1 (right) clearly illustrates this point: our ScalingInter-RL model, with only 7B parameters, achieves an average success rate of approximately $5 8 . 6 \\%$ ater being trained with our reinforcement learning framework. This performance not only surpasses other open-source models of similar size but also significantly outperforms much larger models like Llama3.1-70B ( 47%) and Qwen2.5-72B ( $4 3 \\%$ ), which have nearly ten times the parameters. This demonstrates that the performance improvement gained from simply scaling model size is limited and less steep compared to the gains from targeted post-training and inference-time computation using frameworks like AgentGym-RL. ",
        "bbox": [
            112,
            651,
            883,
            787
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Environmental structure is a key determinant for the efficiency of reinforcement learning. The effectiveness of AgentGym-RL varies depending on the nature of the environment and feedback. In simulated worlds with clear rules and explicit cause-and-effect, like TextCraft, BabyAI, and SciWorld, RL delivers the most significant performance leaps. On SciWorld's complex scientific reasoning tasks, our method boosted the model's score from 1.50 to 50.50, an astounding increase of nearly 50 points. In contrast, for more open-ended environments like WebArena and Deep Search, the performance gains from RL were rather moderate, though still ositive I thesetasks, agents must navigate the complexits real websites, handle multi-step cai plans, or process noisy information from search engines, making it more challenging to lear optimal strategies through trial and error. This suggests that while RL has broad applicability, it excels most in environments where clear feedback and successful pathways can be readily discovered through exploration. ",
        "bbox": [
            112,
            805,
            883,
            911
        ],
        "page_idx": 10
    },
    {
        "type": "discarded",
        "text": "11 ",
        "bbox": [
            490,
            938,
            506,
            949
        ],
        "page_idx": 10
    },
    {
        "type": "image",
        "img_path": "images/7a4a12b57dc5342ffd695f85e1064fd58ac8a6ab243fcb30c52d12a2efe36d55.jpg",
        "image_caption": [
            "Figure7 Training dynamics under different maximum interaction turns in Deep Search environment. Longer-turn s  war  ipan, uapi clapsu but less exploratory learning, leading toa perormanc ceilng.Our ScalngInter-RLmethod progressivlyncse the interaction horizon, and ultimately achieves higher and more efficient long-term performance. "
        ],
        "image_footnote": [],
        "bbox": [
            114,
            98,
            882,
            272
        ],
        "page_idx": 11
    },
    {
        "type": "table",
        "img_path": "images/1568662253a75610dedb8bcc9d162f7347beeb2e74475a8c75d2d30acbadda48.jpg",
        "table_caption": [
            "Tavala reult WeArbenmark.Fo  oup,he tl  boan the s is underlined. In the first row, G & R means GitLab and Reddit. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model</td><td>Shopping</td><td>CMS</td><td>Maps</td><td>G&amp;R</td><td>Overall</td></tr><tr><td colspan=\"6\">Proprietary Models</td></tr><tr><td>GPT-40</td><td>20.00</td><td>13.33</td><td>10.00</td><td>20.00</td><td>16.00</td></tr><tr><td>Qwen-Max</td><td>20.00</td><td>13.33</td><td>20.00</td><td>30.00</td><td>20.00</td></tr><tr><td>Gemini-2.5-Flash</td><td>26.67</td><td>20.00</td><td>10.00</td><td>30.00</td><td>22.00</td></tr><tr><td>OpenAI 04-mini</td><td>33.33</td><td>26.67</td><td>20.00</td><td>70.00</td><td>36.00</td></tr><tr><td>OpenAI o3</td><td>33.33</td><td>0.00</td><td>40.00</td><td>80.00</td><td>34.00</td></tr><tr><td>Gemini-2.5-Pro</td><td>26.67</td><td>26.67</td><td>0.00</td><td>60.00</td><td>28.00</td></tr><tr><td colspan=\"6\">Open-sourced Models ≥ 100B</td></tr><tr><td>Qwen3-235B-A22B</td><td>20.00</td><td>20.00</td><td>20.00</td><td>20.00</td><td>20.00</td></tr><tr><td>DeepSeek-V3-0324</td><td>20.00</td><td>13.33</td><td>10.00</td><td>30.00</td><td>18.00</td></tr><tr><td>DeepSeek-R1-0528</td><td>33.33</td><td>6.67</td><td>30.00</td><td>50.00</td><td>28.00</td></tr><tr><td colspan=\"6\">Open-sourced Models &lt; 100B</td></tr><tr><td>Qwen2.5-3B-Instruct</td><td>13.33</td><td>6.67</td><td>10.00</td><td>10.00</td><td>10.00</td></tr><tr><td>Qwen2.5-7B-Instruct</td><td>14.29</td><td>6.67</td><td>0.00</td><td>16.67</td><td>9.76</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>13.33</td><td>13.33</td><td>0.00</td><td>20.00</td><td>12.00</td></tr><tr><td>Qwen3-4B</td><td>13.33</td><td>6.67</td><td>10.00</td><td>20.00</td><td>12.00</td></tr><tr><td>Qwen3-8B</td><td>20.00</td><td>20.00</td><td>0.00</td><td>10.00</td><td>14.00</td></tr><tr><td>Qwen3-32B</td><td>20.00</td><td>6.67</td><td>20.00</td><td>0.00</td><td>12.00</td></tr><tr><td>Llama-3.1-8B-Instruct</td><td>13.33</td><td>0.00</td><td>20.00</td><td>30.00</td><td>14.00</td></tr><tr><td>Llama-3.1-70B-Instruct</td><td>26.67</td><td>6.67</td><td>20.00</td><td>10.00</td><td>16.00</td></tr><tr><td colspan=\"6\">Our RL Models</td></tr><tr><td>AgentGym-RL-3B</td><td>20.00</td><td>26.67</td><td>10.00</td><td>10.00</td><td>18.00</td></tr><tr><td>AgentGym-RL-7B</td><td>20.00</td><td>33.33</td><td>0.00</td><td>30.00</td><td>22.00</td></tr><tr><td>ScalingInter-7B</td><td>33.33</td><td>26.67</td><td>20.00</td><td>20.00</td><td>26.00</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>",
        "bbox": [
            264,
            410,
            732,
            756
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            112,
            781,
            885,
            827
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "4.3 Detailed Task Performance across Environments ",
        "text_level": 1,
        "bbox": [
            114,
            840,
            593,
            857
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Web navigation. As shown in Table 1, our models demonstrate highly competitive performance on the WebArena benchmark. In particular, the ScalingInter-7B model achieves an overall accuracy of $2 6 . 0 0 \\%$ , significantly surpassing top-tier proprietary models like GPT-40 $( 1 6 . 0 0 \\%$ ) and performing on par with larger models like DeepSeek-R1-0528 $( 2 8 . 0 0 \\% )$ and Gemini-2.5-Pro $( 2 8 . 0 0 \\% )$ . Furthermore, another 7B model of ours, AgentGym-RL-7B, also achieved an overall score of $1 6 . 0 0 \\%$ , matching the performance of GPT-4o. This strong overall performance is underpinned by ScalingInter-7B's state-of-the-art proficiency in structured web navigation, where it achieved scores of $3 3 . 3 3 \\%$ in Shopping and $2 6 . 6 7 \\%$ in CMS, matching the best performance among all models in these categories. However, a significant performance gap remains when compared to the top-performing OpenAI o3 $( 3 4 . 0 0 \\%$ )and o4-mini $( 3 6 . 0 0 \\%$ ), a disparity almost entirely concentrated in the \"GitLab & Reddit\" sub-task. ",
        "bbox": [
            112,
            864,
            887,
            911
        ],
        "page_idx": 11
    },
    {
        "type": "discarded",
        "text": "12 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 11
    },
    {
        "type": "table",
        "img_path": "images/7e3136854ec34f11e10f34ffaf2d36c41a8a954fa13c9bae5ffaabd6a1e1439e.jpg",
        "table_caption": [
            "Ta suD S r oph suln is underlined. SearchR1-it-v0.3 baseline uses Search-R1-v0.3 models[25] "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model</td><td>NQ</td><td>TriviaQA</td><td>PopQA</td><td>HotpotQA</td><td>2Wiki</td><td>Musique</td><td>Bamboogle</td><td>Overall</td></tr><tr><td colspan=\"9\">Proprietary Models</td></tr><tr><td>GPT-40</td><td>20.00</td><td>70.00</td><td>30.00</td><td>30.00</td><td>32.00</td><td>10.00</td><td>34.00</td><td>26.75</td></tr><tr><td>Qwen-Max</td><td>24.00</td><td>52.00</td><td>26.00</td><td>24.00</td><td>16.00</td><td>17.00</td><td>36.00</td><td>29.50</td></tr><tr><td>Gemini-2.5-Flash</td><td>8.00</td><td>60.00</td><td>30.00</td><td>24.00</td><td>16.00</td><td>8.00</td><td>34.00</td><td>23.50</td></tr><tr><td>OpenAI 04-mini</td><td>22.00</td><td>68.00</td><td>50.00</td><td>38.00</td><td>44.00</td><td>28.00</td><td>62.00</td><td>42.50</td></tr><tr><td>OpenAI o3</td><td>28.00</td><td>70.00</td><td>56.00</td><td>46.00</td><td>64.00</td><td>29.00</td><td>74.00</td><td>49.50</td></tr><tr><td>Gemini-2.5-Pro</td><td>22.00</td><td>62.00</td><td>38.00</td><td>28.00</td><td>48.00</td><td>19.00</td><td>56.00</td><td>36.50</td></tr><tr><td colspan=\"9\">Open-sourced Models ≥ 100B</td></tr><tr><td>Qwen3-235B-A22B</td><td>28.00</td><td>54.00</td><td>30.00</td><td>32.00</td><td>22.00</td><td>14.00</td><td>32.00</td><td>28.25</td></tr><tr><td>DeepSeek-V3-0324</td><td>28.00</td><td>60.00</td><td>24.00</td><td>28.00</td><td>18.00</td><td>11.00</td><td>34.00</td><td>26.50</td></tr><tr><td>DeepSeek-R1-0528</td><td>32.00</td><td>68.00</td><td>42.00</td><td>44.00</td><td>50.00</td><td>21.00</td><td>44.00</td><td>40.25</td></tr><tr><td colspan=\"9\">Open-sourced Models &lt; 100B</td></tr><tr><td>Qwen2.5-3B-Instruct</td><td>8.00</td><td>42.00</td><td>22.00</td><td>14.00</td><td>8.00</td><td>2.00</td><td>10.00</td><td>13.50</td></tr><tr><td>Qwen2.5-7B-Instruct</td><td>18.00</td><td>54.00</td><td>20.00</td><td>18.00</td><td>6.00</td><td>4.00</td><td>26.00</td><td>18.75</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>22.00</td><td>52.00</td><td>24.00</td><td>28.00</td><td>24.00</td><td>12.00</td><td>38.00</td><td>26.50</td></tr><tr><td>Qwen3-4B</td><td>18.00</td><td>58.00</td><td>26.00</td><td>24.00</td><td>26.00</td><td>5.00</td><td>20.00</td><td>22.75</td></tr><tr><td>Qwen3-8B</td><td>26.00</td><td>44.00</td><td>26.00</td><td>22.00</td><td>32.00</td><td>10.00</td><td>32.00</td><td>25.25</td></tr><tr><td>Qwen3-32B</td><td>24.00</td><td>54.00</td><td>22.00</td><td>36.00</td><td>28.00</td><td>11.00</td><td>20.00</td><td>25.75</td></tr><tr><td>Llama-3.1-8B-Instruct</td><td>16.00</td><td>26.00</td><td>12.00</td><td>6.00</td><td>2.00</td><td>4.00</td><td>18.00</td><td>11.00</td></tr><tr><td>Llama-3.1-70B-Instruct</td><td>20.00</td><td>44.00</td><td>22.00</td><td>22.00</td><td>18.00</td><td>9.00</td><td>32.00</td><td>22.00</td></tr><tr><td>SearchR1-it-3B-v0.3GRPO</td><td>20.00</td><td>50.00</td><td>30.00</td><td>28.00</td><td>32.00</td><td>5.00</td><td>14.00</td><td>23.00</td></tr><tr><td>SearchR1-it-7B-v0.3GRPO</td><td>24.00</td><td>52.00</td><td>30.00</td><td>22.00</td><td>34.00</td><td>6.00</td><td>26.00</td><td>25.00</td></tr><tr><td colspan=\"9\">Our RL Models</td></tr><tr><td>AgentGym-RL-3B</td><td>30.00</td><td>50.00</td><td>30.00</td><td>30.00</td><td>46.00</td><td>4.00</td><td>12.00</td><td>25.75</td></tr><tr><td>AgentGym-RL-7B</td><td>44.00</td><td>64.00</td><td>32.00</td><td>40.00</td><td>36.00</td><td>15.00</td><td>26.00</td><td>34.00</td></tr><tr><td>ScalingInter-7B</td><td>52.00</td><td>70.00</td><td>46.00</td><td>42.00</td><td>44.00</td><td>14.00</td><td>24.00</td><td>38.25</td></tr></table>",
        "bbox": [
            148,
            137,
            848,
            507
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            112,
            530,
            885,
            637
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Deep search. The evaluation results in Table 2 show the importance of sophisticated reasoning abilities, where proprietary models—particularly the OpenAI 'o' series—currently set the performance benchmark, with OpenAI o3 achieving the highest overal score of 49.50. Against this competitive landscape, our models demonstrate exceptional performance. Specifically, our ScalingInter-7B model achieved an excellent overall score of 38.25, not only surpassing top-tier proprietary models like GPT-4o (26.75) and Gemini-2.5-Pro (36.50) but also performing comparably to the strongest open-source model, DeepSeek-R1-0528 (40.25). Its strengths are particularly salient in key domains: it achieved the highest score overal on the NQ task (52.00) and tied for first place on TriviaQA (70.00) with GPT-4o. Furthermore, our AgentGym-RL-7B (34.00) and AgentGym-RL-3B (25.75) models also delivered strong results, each significantly outperforming open-source counterparts of similar or even larger scales. These results provide strong evidence that our reinforcement learnig apprach effectivey unlocks the model's iherent reasoning capabilities,enabling i to reach or even exceed the performance of elite reasoning models in key scenarios-crucially, without the need for explicit additional long-reasoning. ",
        "bbox": [
            112,
            654,
            883,
            851
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Digital game. The TextCraft benchmark effectively assesses model capabilities across a wide spectrum of difficulty, as detailed in Table 3. At shallow depths (Depth 1), tasks are largely solved by top models. Conversely, the challenge becomes nearly insurmountable at maximum complexity (Depth 4), creating a performance clif for most agents. It is at these intermediate and highest difficulties that the efficacy o our models becomes particularly evident. Our ScalingInter-7B model achieves an outstanding overall score of 91.00, placing it firmly among the top-tier proprietary and large open-source models (93.00-94.00). Critically, it i one of only a few models to achieve a non-zeroscore at Depth 4, scoring 33.33 and demonstratin a nique robustness at maximum complexity. Our AgentGym-RL-7B also excels with a score of 89.00, surpassing prominent models like GPT-4o (83.00). The benefit of our RL training is especially dramatic for smaller models, where AgentGym-RL-3B obtains a score of 75.00, vastly outperforming similarly-sized models like Qwen2.5-3B-Instruct (14.00). These results showcase that our RL approach elevates our models to achieve competitive performance on complex, sequential decision-making tasks. ",
        "bbox": [
            112,
            867,
            885,
            912
        ],
        "page_idx": 12
    },
    {
        "type": "discarded",
        "text": "13 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 12
    },
    {
        "type": "table",
        "img_path": "images/23529eb3edb956403edc2babd0c110568ca41306806c99c68256f9c0635c023d.jpg",
        "table_caption": [
            "TTl o underlined. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model</td><td>Depth 1</td><td>Depth 2</td><td>Depth 3</td><td>Depth 4</td><td>Overall</td></tr><tr><td></td><td colspan=\"5\">Proprietary Models</td></tr><tr><td>GPT-40</td><td>100.00</td><td>87.80</td><td>64.00</td><td>0.00</td><td>83.00</td></tr><tr><td>Qwen-Max</td><td>93.55</td><td>75.61</td><td>36.00</td><td>0.00</td><td>69.00</td></tr><tr><td>Gemini-2.5-Flash</td><td>100.00</td><td>95.12</td><td>40.00</td><td>0.00</td><td>80.00</td></tr><tr><td>OpenAI 04-mini</td><td>100.00</td><td>100.00</td><td>84.00</td><td>0.00</td><td>93.00</td></tr><tr><td>OpenAI o3</td><td>100.00</td><td>100.00</td><td>84.00</td><td>0.00</td><td>93.00</td></tr><tr><td>Gemini-2.5-Pro</td><td>100.00</td><td>100.00</td><td>84.00</td><td>33.33</td><td>94.00</td></tr><tr><td colspan=\"6\">Open-sourced Models ≥</td></tr><tr><td>Qwen3-235B-A22B</td><td>100.00</td><td>100.00</td><td>100B 84.00</td><td>0.00</td><td>93.00</td></tr><tr><td>DeepSeek-V3-0324</td><td>80.65</td><td>53.66</td><td>40.00</td><td>0.00</td><td>57.00</td></tr><tr><td>DeepSeek-R1-0528</td><td>100.00</td><td>100.00</td><td>84.00</td><td>0.00</td><td>93.00</td></tr><tr><td colspan=\"6\">Open-sourced Models &lt; 100B</td></tr><tr><td>Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct</td><td>35.48 80.65</td><td>7.32</td><td>0.00 0.00</td><td>0.00 0.00</td><td>14.00 42.00</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>96.77</td><td>41.46 85.37</td><td>48.00</td><td>0.00</td><td>77.00</td></tr><tr><td>Qwen3-4B</td><td>87.10</td><td>36.59</td><td>12.00</td><td>0.00</td><td>45.00</td></tr><tr><td>Qwen3-8B</td><td>100.00</td><td>78.05</td><td>40.00</td><td>33.33</td><td>74.00</td></tr><tr><td>Qwen3-32B</td><td>90.32</td><td>92.68</td><td>72.00</td><td>33.33</td><td>85.00</td></tr><tr><td>Llama-3.1-8B-Instruct</td><td>74.19</td><td>56.10</td><td>4.00</td><td>0.00</td><td></td></tr><tr><td>Llama-3.1-70B-Instruct</td><td>100.00</td><td>100.00</td><td>84.00</td><td>0.00</td><td>47.00 93.00</td></tr><tr><td colspan=\"6\">Our RL Models</td></tr><tr><td>AgentGym-RL-3B</td><td>100.00</td><td>90.24</td><td>28.00</td><td>0.00</td><td>75.00</td></tr><tr><td>AgentGym-RL-7B</td><td>100.00</td><td>97.56</td><td>72.00</td><td>0.00</td><td>89.00</td></tr><tr><td>ScalingInter-7B</td><td>100.00</td><td>97.56</td><td>76.00</td><td>33.33</td><td>91.00</td></tr></table>",
        "bbox": [
            245,
            137,
            751,
            489
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            112,
            515,
            885,
            651
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Embodied tasks. As demonstrated in Table 4, our RL model achieves state-of-the-art (SOTA) performance on the BabyAI benchmark, with an overall score of 96.67, which is competitive with the leading proprietary models such as o3 and o4-mini. Notably, our ScalingInter-7B model attains the highest overall accuracy of $9 6 . 6 7 \\%$ , outperforming top-tier models such as OpenAI o3 $( 9 4 . 4 4 \\% )$ and GPT-40 $( 8 6 . 6 7 \\%$ ). This exceptional performance is driven by ScalingInter-7B's consistent mastery of diverse sub-tasks, achieving perfect scores of $1 0 0 \\%$ in GoTo, ActionObjDoor (AOD), and SynthLoc, and strong results of $8 0 \\%$ in both FindObjS7 (Find) and OneRoomS20 (Room). Similarly, our AgentGym-RL-7B and AgentGym-RL-3B models demonstrate robust capabilities, reaching overall accuracies of $9 2 . 2 2 \\%$ and $9 3 . 3 3 \\%$ , respectively, and securing perfect scores in GoTo and AOD tasks. Compared to other open-sourced models, such as Qwen3-235B-A22B (87.78%) and DeepSeek-R1-0528 (93.33%), our RL-based models maintain consistently high performance while effectively handling more challenging sub-tasks like Room and Find, where many LLMs exhibit notable variability. Overall, these results highlight the strength of our RL-based approaches, particularly ScalingInter-7B, in achieving state-of-the-art performance on both structured navigation and object-interaction tasks in the BabyAI benchmark. ",
        "bbox": [
            112,
            669,
            883,
            880
        ],
        "page_idx": 13
    },
    {
        "type": "discarded",
        "text": "14 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 13
    },
    {
        "type": "table",
        "img_path": "images/80ae59432244f8e4f1099847066407b54f4d790ba88d7a3485d728b5683323b9.jpg",
        "table_caption": [
            "Tavala esult BabI bncmark a roupetresul  oanhe sn underlined. In the frst row, AOD means ActionObjDoor, Find means FindObjS7, Room means OneRoomS20, SLoc means SynthLoc. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model</td><td>GoTo</td><td>Pickup</td><td>AOD</td><td>Find</td><td>Room</td><td>SLoc</td><td>Overall</td></tr><tr><td colspan=\"6\">Proprietary Models</td><td></td></tr><tr><td>GPT-40</td><td>92.73</td><td>80.00</td><td>100.00</td><td>80.00</td><td>60.00</td><td>60.00</td><td>86.67</td></tr><tr><td>Qwen-Max</td><td>92.73</td><td>80.00</td><td>80.00</td><td>60.00</td><td>60.00</td><td>80.00</td><td>85.56</td></tr><tr><td>Gemini-2.5-Flash</td><td>92.73</td><td>86.67</td><td>80.00</td><td>20.00</td><td>60.00</td><td>100.00</td><td>85.56</td></tr><tr><td>OpenAI 04-mini</td><td>96.36</td><td>100.00</td><td>100.00</td><td>80.00</td><td>40.00</td><td>80.00</td><td>92.22</td></tr><tr><td>OpenAI o3</td><td>98.18</td><td>93.33</td><td>100.00</td><td>80.00</td><td>60.00</td><td>100.00</td><td>94.44</td></tr><tr><td>Gemini-2.5-Pro</td><td>94.55</td><td>93.33</td><td>100.00</td><td>40.00</td><td>60.00</td><td>60.00</td><td>87.77</td></tr><tr><td colspan=\"8\">Open-sourced Models</td></tr><tr><td>Qwen3-235B-A22B</td><td>89.09</td><td>86.67</td><td>100.00</td><td>80.00</td><td>60.00</td><td>100.00</td><td>87.78</td></tr><tr><td>DeepSeek-V3-0324</td><td>67.27</td><td>53.33</td><td>0.00</td><td>20.00</td><td>40.00</td><td>60.00</td><td>56.67</td></tr><tr><td>DeepSeek-R1-0528</td><td>98.18</td><td>86.67</td><td>100.00</td><td>60.00</td><td>80.00</td><td>100.00</td><td>93.33</td></tr><tr><td colspan=\"8\">Open-sourced Models</td></tr><tr><td>Qwen2.5-3B-Instruct</td><td>61.82</td><td>40.00</td><td>20.00</td><td>60.00</td><td>40.00</td><td>20.00</td><td>52.22</td></tr><tr><td>Qwen2.5-7B-Instruct</td><td>70.91</td><td>66.67</td><td>60.00</td><td>80.00</td><td>60.00</td><td>20.00</td><td>66.67</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>92.73</td><td>93.33</td><td>100.00</td><td>60.00</td><td>60.00</td><td>80.00</td><td>88.89</td></tr><tr><td>Qwen3-4B</td><td>60.00</td><td>60.00</td><td>40.00</td><td>40.00</td><td>40.00</td><td>20.00</td><td>54.44</td></tr><tr><td>Qwen3-8B</td><td>43.64</td><td>20.00</td><td>40.00</td><td>40.00</td><td>40.00</td><td>40.00</td><td>38.89</td></tr><tr><td>Qwen3-32B</td><td>87.27</td><td>80.00</td><td>100.00</td><td>60.00</td><td>40.00</td><td>80.00</td><td>82.22</td></tr><tr><td>Llama-3.1-8B-Instruct</td><td>85.45</td><td>60.00</td><td>100.00</td><td>80.00</td><td>60.00</td><td>40.00</td><td>77.78</td></tr><tr><td>Llama-3.1-70B-Instruct</td><td>89.09</td><td>86.67</td><td>100.00</td><td>60.00</td><td>60.00</td><td>100.00</td><td>86.67</td></tr><tr><td colspan=\"8\">Our RL Models</td></tr><tr><td>AgentGym-RL-3B</td><td>100.00</td><td>100.00</td><td>100.00</td><td>60.00</td><td>60.00</td><td>60.00</td><td>93.33</td></tr><tr><td>AgentGym-RL-7B</td><td>100.00</td><td>93.33</td><td>100.00</td><td>60.00</td><td>60.00</td><td>60.00</td><td>92.22</td></tr><tr><td>ScalingInter-7B</td><td>100.00</td><td>93.33</td><td>100.00</td><td>80.00</td><td>80.00</td><td>100.00</td><td>96.67</td></tr></table>",
        "bbox": [
            187,
            151,
            810,
            531
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Scientific Scenario. Our experiments on the SciWorld benchmark, summarized in Table 5, demonstrate the advanced performance of our RL-trained models. Our ScalingInter-7B model establishes a new state-of-the-art with an overall score of 57.00, which significantly surpasses al open-source and proprietary models, incuding the next-best proprietary model, OpenAI o3 (41.50). This superior performance is primarily atributed to high scores in the \"Find\" (88.64) and \"Test-Cond\" (55.42) sub-tasks. Furthermore, our AgentGym-RL-7B model also shows strong capabilities, securing the second-highest overall score (50.50) and achieving the top score in \"Test-Cond\" (59.04). These results highlight the effectiveness of our RL method for training agents in exploration and procedural execution tasks. However, our findings also identify a critical limitation shared across all evaluated models. The \"Chem-Mix\" sub-task proved to be intractable, with every model, including or op perorers, scor zroThis iorm resul idicat  ystehallenge o cret models in tasks requiring complex scientific reasoning and multi-step chemical simulation, marking this as a crucial area for future research. ",
        "bbox": [
            112,
            555,
            883,
            736
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "5 Discussion ",
        "text_level": 1,
        "bbox": [
            114,
            750,
            248,
            767
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "5.1 Test-Time Scaling for Agents ",
        "text_level": 1,
        "bbox": [
            112,
            779,
            416,
            796
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "In this subsection, we investigate how agent performance changes as inference compute increases. ",
        "bbox": [
            111,
            804,
            808,
            819
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Scaling sequential inter action. First, we study how performance changes when the maximum number of interaction turns available to the model is raisedAs shown in Figure, all modes exhibit clear gains as the number of turns increases, which validates the insight behind our ScalingInter-RL approach—namely, agents must thoroughly explore the environment to shape their interaction and behavioral patterns. Furthermore, our trained agent consistently outperforms the baseline by a significant margin, further demonstrating the ",
        "bbox": [
            112,
            837,
            885,
            912
        ],
        "page_idx": 14
    },
    {
        "type": "discarded",
        "text": "15 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 14
    },
    {
        "type": "table",
        "img_path": "images/2d4009e006534b967409b7569172d1ff146247f126e300a5f779e3ae65d42fff.jpg",
        "table_caption": [
            "TulSW u underlined. In the first row, Test-Cond. means test-conductivity, Chem-Mix means chemistry-mix. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model</td><td>Measure</td><td>Test-Cond.</td><td>Find</td><td>Chem-Mix</td><td>Lifespan</td><td>Overall</td></tr><tr><td colspan=\"7\">Proprietary Models</td></tr><tr><td>GPT-40</td><td>15.09</td><td>6.02</td><td>38.64</td><td>20.00</td><td>73.33</td><td>21.00</td></tr><tr><td>Qwen-Max</td><td>9.43</td><td>0.00</td><td>34.09</td><td>20.00</td><td>40.00</td><td>13.50</td></tr><tr><td>Gemini-2.5-Flash</td><td>11.32</td><td>0.00</td><td>54.55</td><td>0.00</td><td>80.00</td><td>21.00</td></tr><tr><td>OpenAI 04-mini</td><td>20.75</td><td>14.46</td><td>47.73</td><td>0.00</td><td>100.00</td><td>29.50</td></tr><tr><td>OpenAI o3</td><td>47.17</td><td>25.30</td><td>56.82</td><td>40.00</td><td>66.67</td><td>41.50</td></tr><tr><td>Gemini-2.5-Pro</td><td>9.43</td><td>0.00</td><td>29.55</td><td>0.00</td><td>46.67</td><td>12.50</td></tr><tr><td colspan=\"7\">Open-sourced Models ≥ 100B</td></tr><tr><td>Qwen3-235B-A22B</td><td>11.32</td><td>4.82</td><td>59.09</td><td>20.00</td><td>66.67</td><td>23.50</td></tr><tr><td>DeepSeek-V3-0324</td><td>0.00</td><td>0.00</td><td>2.27</td><td>0.00</td><td>0.00</td><td>0.50</td></tr><tr><td>DeepSeek-R1-0528</td><td>1.89</td><td>0.00</td><td>11.36</td><td>0.00</td><td>20.00</td><td>4.50</td></tr><tr><td colspan=\"7\">Open-sourced Models &lt; 100B</td></tr><tr><td>Qwen2.5-3B-Instruct</td><td>3.77</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>1.00</td></tr><tr><td>Qwen2.5-7B-Instruct</td><td>1.89</td><td>0.00</td><td>0.00</td><td>0.00</td><td>13.33</td><td>1.50</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>7.55</td><td>1.20</td><td>15.91</td><td>20.00</td><td>40.00</td><td>9.50</td></tr><tr><td>Qwen3-4B</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>33.33</td><td>2.50</td></tr><tr><td>Qwen3-8B</td><td>9.43</td><td>0.00</td><td>18.18</td><td>0.00</td><td>46.67</td><td>10.00</td></tr><tr><td>Qwen3-32B</td><td>5.66</td><td>1.20</td><td>31.82</td><td>0.00</td><td>66.67</td><td>14.00</td></tr><tr><td>Llama-3.1-8B-Instruct</td><td>9.43</td><td>0.00</td><td>4.55</td><td>20.00</td><td>0.00</td><td>4.00</td></tr><tr><td>Llama-3.1-70B-Instruct</td><td>24.53</td><td>4.82</td><td>40.91</td><td>40.00</td><td>86.67</td><td>25.00</td></tr><tr><td colspan=\"7\">Our RL Models</td></tr><tr><td>AgentGym-RL-3B</td><td>20.75</td><td>28.92</td><td>0.00</td><td>0.00</td><td>66.67</td><td>22.50</td></tr><tr><td>AgentGym-RL-7B</td><td>24.53</td><td>59.04</td><td>65.91</td><td>0.00</td><td>66.67</td><td>50.50</td></tr><tr><td>ScalingInter-7B</td><td>33.96</td><td>55.42</td><td>88.64</td><td>0.00</td><td>73.33</td><td>57.00</td></tr></table>",
        "bbox": [
            187,
            136,
            810,
            500
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "effectiveness of our method. ",
        "bbox": [
            114,
            525,
            313,
            540
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Scaling parallel sampling. As shown in Figure 9, increasing the number of samples yields a marked improvement in Pass@K performance, signaling the downstream optimization potential of each model. Our model surpasses the baselines even with a smal sampling budget, and as sampling increases, it continues to outperform the baseline in a stable and significant manner: for example, with 64 sampling attempts, our RL model achieves a $5 . 5 \\%$ improvement in Deep Search environment and a $7 . 0 5 \\%$ improvement in SciWorld environment over the untrained base model, showcasing its superior optimization capability. ",
        "bbox": [
            112,
            556,
            883,
            648
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "5.2 Performance of Different RL Algorithm ",
        "text_level": 1,
        "bbox": [
            114,
            661,
            501,
            678
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "We comprare two mainstream RL algorithms for LLM post-training, i.e., GRPO and REINFORCE++. As shown in Table 6, our experiments reveal that GRPO consistently and substantially outperforms REINFORCE $^ { + + }$ on the TextCraft, BabyAI, and Deep Search benchmarks. While model scaling from 3B to 7B parameters improves results for both algorithms, the superiority of GRPO is particularly stark: its 3B variant achieves higher scores than the 7B REINFORCE++ model. This finding points to a fundamental algorithmic advantage that is more impactful than sheer model scale. ",
        "bbox": [
            112,
            685,
            885,
            776
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "The performance difference can be explained by how each algorithm handles the core difficulties of these tasks: vast exploration spaces and sparse rewards. The learning signal for REINFORCE++ is derived from full-episodeMonteCarloreturns, whic ote results ihig-variancegradients that are sensitivetotoasic succeses and failures over long trajectories. In contrast, GRPO mitigates this instability by evaluating the relative merit of actions against a learned baseline. The focus on action advantage provides a more stable and robust gradient, facilitating more efficient exploration and credit assignment in complex, low-signal environments. ",
        "bbox": [
            112,
            784,
            883,
            890
        ],
        "page_idx": 15
    },
    {
        "type": "discarded",
        "text": "16 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 15
    },
    {
        "type": "image",
        "img_path": "images/c4b1703d01ca3081d1fdd83c1f49bc6c5da0fcc7b92f64e2181ee0625c404bc7.jpg",
        "image_caption": [
            "Figure 8 Scaling test interaction turns. "
        ],
        "image_footnote": [],
        "bbox": [
            114,
            97,
            883,
            320
        ],
        "page_idx": 16
    },
    {
        "type": "image",
        "img_path": "images/d0ddc042725cfda72f792953dfa787a22351867bdca07f67a5aeea99883fa888.jpg",
        "image_caption": [
            "Figure 9 Pass@K performance. "
        ],
        "image_footnote": [],
        "bbox": [
            114,
            363,
            883,
            588
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "5.3 Case Study ",
        "text_level": 1,
        "bbox": [
            112,
            641,
            261,
            657
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "In this section, we present a series of case studies that highlight both the shortcomingsof the baseagent and the improvements achieved by our reinforcement learning models. Across navigation, compositional problem solving, and web interaction senarios, the RL-trained agents consistently overcame unproductive behavioral loops, exhibited adaptive recovery strategies, and demonstrated more systematic task execution. To provide a balanced perspective, we also include two representative failure cases—in scientific reasoning and in efficent web interaction—that underscore areas where further refinement is needed. In the main text, we showcase the WebArenatrajectoryillustration and its correspondingvisualization, whiladditional trajectoryilustrations for other environments are provided in Appendix C. ",
        "bbox": [
            112,
            665,
            883,
            786
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Enhanced navigation. Figure 12 demonstrates a notable improvement in navigation capabilities within BabyAI environment. While the base agent exhibited suboptimal behavior characterized by repetitive movement patterns-going through previously explored locations without developing a strong search strategy for completion-the model trained throug reinorcement learning manifested moreeffectiveexploration straty. The RL agent demonstrated strategic backtracking capabilities, systematically exiting throug doorways before selecting alternative pathways, ultimately accessing a green door that provided direct access to the target blue box. This highlights the RL model's superior ability in spatial reasoning and its ability to circumvent ",
        "bbox": [
            112,
            804,
            883,
            910
        ],
        "page_idx": 16
    },
    {
        "type": "discarded",
        "text": "17 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 16
    },
    {
        "type": "table",
        "img_path": "images/1fe91ed726b3b5d79f23214fcb15e645ca65f4f640bb9d97d99488cffc9f2f93.jpg",
        "table_caption": [
            "Table 6 Evaluation results of different RL algorithms. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>RL Algorithms TextCraft BabyAl SearchQA</td><td></td><td></td><td></td></tr><tr><td colspan=\"3\">Qwen2.5-3B-Instruct</td><td></td></tr><tr><td>GRPO</td><td>75.00</td><td>93.33</td><td>25.75</td></tr><tr><td>REINFORCE++</td><td>28.00</td><td>70.00</td><td>13.25</td></tr><tr><td colspan=\"3\">Qwen2.5-7B-Instruct</td><td></td></tr><tr><td>GRPO</td><td>83.00</td><td>92.22</td><td>34.00</td></tr><tr><td>REINFORCE++</td><td>73.00</td><td>84.44</td><td>24.00</td></tr></table>",
        "bbox": [
            310,
            121,
            692,
            253
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "unproductive behavioral loops. ",
        "bbox": [
            114,
            279,
            334,
            292
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Compositional Task Mastery. Figure 14 exemplifes the successful application of reinforcement learning to complex scientific task execution. The base agent exhibited fundamental deficiencies in task interpretation, misusing non-interactive objects and generating invalid actions. In contrast, the RL-optimized agent demonstrated comprehensive task understanding through its systematic approach: correctly identifying and manipulating a living thing (the banana tree), executing appropriate inventory management operations, navigating multi-room environments with obstacle resolution capabilities and successfully completing the objective by depositing the tree in the designated purple box. This highlights the RL agent's enhanced capabilities in reasoning, planning, and sequential task execution within compositional problem spaces. ",
        "bbox": [
            112,
            310,
            885,
            431
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Adaptive Web Navigation Strategies. Figure 10 and figure11 illustrates the emergence of web navigation capabilities through reinforcement learning optimization. The base agent persistently interacted with nonresponsive interface elements, specifically engaging in repetitive clicking behaviors on inefective targets without recognizing the futility of these actions. Our RL-trained agent exhibited markedly superior adaptive behavior: it successfully implemented error recovery mechanisms when encountering a \"Page not found\" euizihe r oate pur\"enyictuay content within trending posts, and completing the subscription task successfully—demonstrating enhanced robustness in error handling, purposeful navigation strategies, and the ability to maintain task focus while avoiding unproductive behavioral patterns. ",
        "bbox": [
            112,
            449,
            883,
            585
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Limitations in Scientific Scenario. Figure 15 reveals fundamental procedural execution failures that persist in SciWorld task completion despite the RL agent's ability to reach task-relevant game states. These instances exemplify two distinct failure modalitiesfrst, when confronted withinteraction failures requiring systematicdebugging, theagent nappropriatelysubstitute direct actualrecll o thentended expermental procedure; second, the agent demonstrates insuficient systematic exploration, as evidenced by its premature task termination after navigating to the outdoor environment and focusing only on the chameleon egg rather than analyzingal availableanimals that the task demandsThese failure clectivelyidicate that theode lacks the deep procedural understanding necessary for executing rigorous scientific comparative analyses. ",
        "bbox": [
            112,
            602,
            883,
            723
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Over-Interaction Patterns in Web Navigation. Figure 16 demonstrates a prevalent failure mode of excessive and inefficient interaction sequences during web navigation tasks. Despite successfully navigating to the correct target websites in both illustrated cases, the RL agent engages in superfuous interaction patterns—including redundant clicking, unnecessary hovering, and excessive scrolling—that impede successful information extraction from the target pages. These behavioral patterns suggest that the reinforcement learning process failed to instil the precision and effiency required for optimal task completion, indicating a gap between state-reaching capabilities and effective action selection within those states. ",
        "bbox": [
            112,
            741,
            885,
            847
        ],
        "page_idx": 17
    },
    {
        "type": "discarded",
        "text": "18 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "WebArena ",
        "text_level": 1,
        "bbox": [
            455,
            109,
            542,
            125
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Task: Open the thread of a trending post on the forum \"pittsburgh\" and subscribe.   \nObs 0: URL: http://reddit.com/ Tab 0 (current): Postmill [1] RootWebArea 'Postmill' focused: True [232] link 'Forums' [233] link 'Wiki' [241] searchbox 'Search query' [242] link 'Notifications (0)' [243] link 'Submit' ",
        "bbox": [
            171,
            138,
            790,
            219
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Base Model ",
        "text_level": 1,
        "bbox": [
            271,
            239,
            369,
            252
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Our RL Model ",
        "text_level": 1,
        "bbox": [
            617,
            239,
            735,
            252
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Thought 1: …We need to navigate to the \"pittsburgh\" forum first.To do this, we can click on the \"Forums\" link in the header.   \nAct 1: click [232]   \nObs 1: .[1958] searchbox 'Search query' Thought 2: .…we can see that \"AskReddit\" and   \n\"relationship_advice\" are listed, but \"pittsburgh\" is not. We need to search for the \"pittsburgh\" forum using the search box.   \nAct 2: type [1958] [pittsburgh] [1]   \nObs 2: .[2951] link 'From Florida to Pittsburgh : Is \\$100,000 home budget in Pittsburgh on disability income feasibl'… Thought 1: .…We need to navigate to the \"pittsburgh\" forum first.   \nAct 1: click [66]   \nObs 1: \"Page not found\"   \nThought 2: …the current page is a \"Page not found\" error page. We need to navigate back to the homepage or the main page of the website to find the   \n\"pittsburgh\" forum.   \nAct 2: go_back   \nObs 2: .   \nThought 3: … there is a link to the \"Forums\" section which we can use to navigate to the \"pittsburgh\"   \nforum.   \nAct 3: click [65]   \nObs 3: . [500] searchbox 'Search query' …   \nThought 4: The current page lists various forums, but there is no forum named \"pittsburgh\" listed. We need to search for the \"pittsburgh\" forum.   \nAct 4:type [500] [pittsburgh] [1]   \nObs 4: [4004] link 'From Florida to Pittsburgh : Is \\$100,000 home budget in Pittsburgh on disability   \nincome feasible?'   \nThought 5: …The search results for \"pittsburgh\" are now displayed, and the first post seems to be a   \ntrending post.   \nAct 5:click [4004]   \nObs 5: ..[8484] button 'Subscribe No subscribers'… Thought 6: The objective is to open the thread of a trending post on the forum \"pittsburgh\" and subscribe. Act 6: click [8484]   \nObs 6:[8484] button 'Unsubscribe 1 subscriber'   \nThought 7: The subscription button is now showing that there is 1 subscriber, indicating that the user is already subscribed.   \nAct 7: stop [N/A]   \nScore: 1.0 ",
        "bbox": [
            166,
            262,
            464,
            310
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            166,
            319,
            467,
            396
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            519,
            261,
            826,
            686
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "6 Related Work ",
        "text_level": 1,
        "bbox": [
            114,
            738,
            279,
            756
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Developing agents with large language models. With the advancement of large language models [1, 2, 65], researchers have begun using them to build agents capable of multi-turn intelligent decision-making [79, 86]. The predominant approaches rely on prompting to guide the model to invoke tools [48, 90], augmented wih mechani such as sel-refection [51, 57, 78, 82], ln-horizon plannng [36, 40, 45, 61], and sel-corein [29, 31]. Some work constructs workflows that assign different roles to multiple LLMs [17, 20, 34, 64, 74], each playing a specialized part in task completion. However, these methods typically depend on powerful proprietary models (e.g., OpenAI o3) and do not train the underlying models to evolve into agents with intriicagentic capabilites.Anotherne  work gathers arge-scaleexpert trajectories and trains agentst mimic experts step by step [4, 6, 91, 92], thereby acquiring abilities such as API invocation, planning, and elf-refectin However, this approach is expensive difficult to scale, and the model struggles to sel-improve hrough interactions with the environment. ",
        "bbox": [
            112,
            768,
            883,
            905
        ],
        "page_idx": 18
    },
    {
        "type": "discarded",
        "text": "19 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 18
    },
    {
        "type": "image",
        "img_path": "images/c58cd9f851991f887343e361e27e498ff7166db236b795a35a78422863965d07.jpg",
        "image_caption": [
            "Figure 11 Trajectory visualization of our RL agent on Webarena task. "
        ],
        "image_footnote": [],
        "bbox": [
            147,
            106,
            849,
            577
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            117,
            640,
            883,
            670
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Reinforcement learning for large language model agents. Reinforcement learning has become a crucial post-training technique for large language models, enabling alignment with human preferences [, 24, 43, 80, 93], ehancingreasoni capabilities [12, 18, 2, 49, 68, 76], an serving as a new scalin dimnsn [12]. Representative algorithms include PPO [53], GRPO [54], REINFORCE++ [21], RLOO [30], and others. However, most existing works—such as DeepSeek-R1—are confined to single-turn, static tasks in which models do not engage in multi-turn interactions with complex environments. Recent work has used RL to train agents for self-reflection [82], tool use [89], and even long-horizon interactions [3, 26, 30, 47, 71, 95]. However, these methods often struggle with scalable deployment due to limited task complexity and environment diversity, and they frequently encounter optimization instability that hinders performance. To overcome these challenges, we propose a unified, end-to-end RL framework spanning diverse real-world environments for training models in multi-turn decision-making without requiring SFT as a preliminary step. We further intrduce ScalingInter-RL,aninteraction-scalintechnique that stabilizes ptiization andboosts theagent' final performance. ",
        "bbox": [
            112,
            688,
            883,
            883
        ],
        "page_idx": 19
    },
    {
        "type": "discarded",
        "text": "20 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Scaling Inference Compute for language models. OpenAI o1 and DeepSeek-R1 have shown that increasing compute during inference (both at test time and during RL rollouts) can yield strong scaling effects [12, 23, 59, 75]. Researchers have also explored various approaches to achieve similar gains—such as long-chain-of-thought reasoning [59, 78], majority voting [33, 70], best-of-N sampling [9, 27], beam search [81, 97], and Monte Carlo tree search [8, 16]. However, in the field of LLM agents, few works discuss how to scale inference compute. Zhu et al. [96] explore various test-time scaling strategies in agents and achieve significant gains, yet they do not investigate inference scaling in RL. The closest work may be TTI [55], which uses rejection sampling to teach agents to allocate more compute in interactions instead of thinking on web navigation tasks. In contrast, our approach employs mainstream on-policy RL algorithms—such as GRPO and REINFORCE++—and does not constrain the agent to use compute only for thinking or acting. Instead, we simply scale interactions and let the model decide how best to allocate its additional compute. Our method progressively grants the agent more exploration capacity, enabling it over time to better adapt to its environment, acquire more complex skills, and master more challenging tasks. ",
        "bbox": [
            112,
            98,
            885,
            295
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "7 Conclusion and Future Work ",
        "text_level": 1,
        "bbox": [
            112,
            310,
            419,
            328
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "In this work, we introduced AgentGym-RL, a novel and unified reinforcement learning framework designed to train LLM-based agents with long-horizon, multi-turn decision-making capabilities. The framework features diverse environments and tasks, supports mainstream RL algorithms, and is highly extensible—offering the community a practical and powerful toolkit. Additionally, we proposed the ScalingInter-RL method to prgressively enhance agentsinteractive inteligence in astagedmanner. Extensiv experiments demonstrate the effectiveness of both the framework and the method. However, several important directions remain for future exploration: ",
        "bbox": [
            112,
            340,
            885,
            446
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Developing agents with generalization and transfer capabilities. Currently, our trained agents perform well within in-domain settings.A key challenge going forward is to enable agents to adapt seamlessly to novel environments and unfamiliar tools while maintaining high performance. ",
        "bbox": [
            114,
            463,
            883,
            508
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Scaling RL training to longer-horizon and more realistic, physically grounded tasks. Most existing studies—including ours—focus on relatively simple digital tasks. However, real-world tasks are often longer-horizon, more complex, and grounded in the physical world. These tasks demand that agents process richer sensory inputs and reason over significantly larger action spaces, introducing new challenges for both reinforcement learning training and test-time interaction. ",
        "bbox": [
            112,
            525,
            883,
            602
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Advancing multi-agent reinforcement learning. Our current framework primarily targets single-agent training. However, multi-agent architectures open up new possibilities and may lead to stronger performance. At the same time, they introduce aditionaluncertainty and pose greater demands on bothinfrastructure and algorithm design. ",
        "bbox": [
            114,
            619,
            885,
            680
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Acknowledgement ",
        "text_level": 1,
        "bbox": [
            114,
            695,
            297,
            713
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "This work was supported by Huawei Ascend AI processors. We sincerely thank Huawei for providing the computing resources that made this research possible. ",
        "bbox": [
            112,
            724,
            885,
            756
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "References ",
        "text_level": 1,
        "bbox": [
            114,
            771,
            223,
            787
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ige Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[2] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 1(1):4, 2024.   \n[3] Shiyi Cao, Sumanth Hegde, Dacheng Li, Tyler Griggs, Shu Liu, Eric Tang, Jiayi Pan, Xingyao Wang, Akshay Malik, Graham Neubig, Kourosh Hakhamaneshi, Richard Liaw, Philipp Moritz, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. Skyrl-v0: Train real-world long-horizon agents via reinforcement learning, 2025.   \n[4] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Colier, Karthik Narasimhan, and Shunyu Yao. Fireact: Toward language agent fine-tuning. CoRR, abs/2310.05915, 2023. doi: 10.48550/ARXIV.2310.05915. URL https://doi.org/10.48550/arXiv.2310.05915.   \n[5] Lu Chen, Rui Zheng,Bingi Wang, Senje Jin, Caisu Huan, Junje e, Zhi Zhang, Yuho Zhou, Ziheg Xi, Tao Gui, Qi Zhang, and Xuanjing Huang. Improving discriminative capability of reward models in RLHF using contrastive learning. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 1527015283. Association for Computational Linguistics, 2024. doi: 10.18653/V1/ 2024.EMNLP-MAIN.852. URL https://doi.org/10.18653/v1/2024.emnlp-main.852.   \n[6] Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. Agan Desii atand method  eive agentuig or largeanggemodls. In Lun-Wei Ku, Ande Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 93549366. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-ACL.557. URL https://doi.org/10.18653/v1/2024.fin dings-acl.557.   \n[7] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nyen,anYshuBengBabya plator  stuye plnc  o languae lar. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=rJeXCoOcYX.   \n[8] Yizhou Chi, Yizhang Lin, Sirui Hong, Duyi Pan, Yaying Fei, Guanghao Mei, Bangbang Liu, Tianqi Pang, Jacky Kwok, Ceyao Zhang, Bang Liu, and Chenglin Wu. SELA: tree-search enhanced LLM agents for automated machine learning. CoRR, abs/2410.17238, 2024. doi: 10.48550/ARXIV.2410.17238. URL https://doi.org/10.4 8550/arXiv.2410.17238.   \n[9] Yinlam Chow, Guy Tennenholtz, Izzeddin Gur, Vincent Zhuang, Bo Dai, Aviral Kumar, Rishabh Agarwal, Sridhar Thiagarajan, Craig Boutilier, and Aleksandra Faust. Inference-aware fine-tuning for best-of sampling in large language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=77gQUdQhE7.   \n[10] Gheorghe Comanici, Eric Bieber, Mike Schakermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025.   \n[11] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haoeg Wang, Haowei Zhang, Honghui Ding, Huajan Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jao Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jinghag Chen, Jingang Yuan, Junje Qiu, Juno Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengeng Ye, Shengeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shuneng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, and Wangding Zeng. Deepseek-v3 technical report.CoRR, abs/2412.19437, 2024. doi: 10.48550/ARXIV.2412.19437. URL https://doi.org/10.48550/arXiv.2412.19437.   \n[12] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokng Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyag Yuan, Junje Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pn, Runji Wang, R. J.Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengeng ",
        "bbox": [
            119,
            799,
            888,
            914
        ],
        "page_idx": 20
    },
    {
        "type": "discarded",
        "text": "21 ",
        "bbox": [
            490,
            938,
            506,
            949
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            109,
            36,
            893,
            909
        ],
        "page_idx": 21
    },
    {
        "type": "discarded",
        "text": "22 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Ye, Shiyu Wang, Shuiping u, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning ",
        "bbox": [
            143,
            99,
            887,
            141
        ],
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "capability in lms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi: 10.48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948.   \n[13] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samual Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36:Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/5950bf290a1570ea401bf9882128160-A bstract-Datasets_and_Benchmarks.html.   \n[14] Abhimanyu Dubey, Abhinav Juhri, Abhinav Pandey, Abhishek Kadian, Ahmad A-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurel Hailey Nguyen, Hanna Korevaar, Hu Xu, Hugo Touvron, Iiyan Zarov, Imanol Arrieta Iarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Janfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang JiecaoYu, JoannaBitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe Junteng Ja, KalyanVasudenAlwala,KartikeyaUpasani, Kate Plawak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https://doi.org/10.48550/arXiv.2407.21783.   \n[15] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips . cc/pap er_files/paper/2022/hash/74a67268c5cc5910f64938cac4526a90-Abstract-Datasets_and_Benchmarks.html.   \n[1] Bingheng Gan, Yufan Zhao, Tianyi Zhang, Jing Huang, Yusu Li, Shu Xian Teo, Changwang Zhang, and Wei Shi. MATER uiagent tem wi LLM ea MCTS. In Li Chizz, Alan Rit, and L Wa, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pages 9409-9426. Association for Computational Linguistics, 2025. doi: 10.18653/V1/2025.NAACL-LONG.476. URL https://doi.org/10.18653/v1/2025.naacl-1ong.476.   \n[17] Honglin Guo, Kai Lv, Qipeng Guo, Tianyi Liang, Zhiheng Xi, Demin Song, Qi Zhang, Yu Sun, Kai Chen, Xipeng Qiu, and Tao Gui. Critiq: Mining data quality criteria from human preferences. CoRR, abs/2502.19279, 2025. doi: 10.48550/ARXIV.2502.19279. URL https://doi.org/10.48550/arXiv.2502.19279.   \n[18] Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, and Yahui Zhou. Skywork open reasoner 1 technical report. CoRR, abs/2505.22312, 2025. doi: 10.48550/ARXIV.2505.22312. URL https://doi.org/10.48550/arXiv.2505.22312.   \n[19] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and AkikoAizawa. Constructing A multi-hop QA dataset for cehensive evaluati  reasning steps. In Doni Scott, Núria Bel, and Chengqig Zong, editors, Procngs of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 6609-6625. International Committee on Computational Linguistics, 2020. doi: 10.18653/V1/2020.COLING-MAIN.580. URL https://doi.org/10.18653/v1/2020.coling-main.580.   \n[20] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven KaShing Yau, Zijuan Lin, Liyang Zhou, ChenyuRan, Lingfeng Xiao, Chengln Wu, and Jürge Schmiuber. Metagpt: Meta programming for A multi-agent colaborative framework. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=VtmBAGCN7o.   \n[21] Jian Hu. REINFORCE++: A simple and effcient approach for aligning large language models. CoRR, abs/2501.03262, 2025. doi: 10.48550/ARXIV.2501.03262. URL https://doi.org/10.48550/arXiv.2501.03262.   \n[22] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024.   \n[23] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed E-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024.   \n[24] Jiaming Ji, Tianyi Qiu, Boyun Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan Yee Ng, Juntao Dai, Xuehai Pan, Aidan O'Gara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, Stephen McAler, Yaodong Yang, Yizhou Wang, Song-Chun Zhu, Yike Guo, and Wen Gao. AI alignment: A comprehensive survey. CoRR, abs/2310.19852, 2023. doi: 10.48550/ARXIV.2310.19852. URL https://doi.org/10.48550/arXiv.2310.19852.   \n[5 n, Jon r   Han.u learning for reasoning-search interleaved LLM agents. CoRR, abs/2505.15117, 2025. doi: 10.48550/ARXIV.2505. 15117. URL https://doi.org/10.48550/arXiv.2505.15117.   \n[26] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. CoRR, abs/2503.09516, 2025. doi: 10.48550/ARXIV.2503.09516. URL https://doi.org/10.48550/arXiv.2503.09516.   \n[27 Yu Jinnai, TetsuroMorimura, Kaito Ariu, and Kenshi Abe. Regularized besto- sampling to mitigate reward hacking for language model alignment. CoRR, abs/2404.01054, 2024. doi: 10.48550/ARXIV.2404.01054. URL https://doi.org/10.48550/arXiv.2404.01054.   \n[8 Mandar Joshi, Eunsol Choi, Daniel Weld,andLuke Zettleor.Triviqa:large saleistany uevised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 16011611. Association for Computational Linguistics, 2017. doi: 10.18653/V1/P17-1147. URL https://doi.org/10.18653/v1/P17-1147.   \n[29] Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. When can llms Actually correct their own mistake critil survey f se-coectionof lm.Trans.Asso.Coput. Linguistic, 1:14171440, 20: 10.1162/TACL\\_A\\_00713. URL https://doi.org/10.1162/tacl_a_00713.   \n[30] Wouter Kool, Herke van Hoof, and Max Welng. Buy 4 REINFORCE samples, get a baseline for free! In Deep Reinforcement Learning Meets Structured Prediction, ICLR 2019 Workshop, New Orleans, Louisiana, United States, May 6, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=r11gTGL5DE.   \n[1AviralKumar Vincet Zhua Risha garal, Yi Su, JohnDo-Reye, Avi Singh, KateBau Sharal, Colton Bishop, Rebecca Roelofs, Lei M. Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucke, DoiPrecu, Feryal . Behahan an Aleksandra Faust.Traing language modes t sel-cor via reinforcement learning. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=CjwERcAU7w.   \n[ToKowski Jeloa OliviReMicl ColnsAnkr. ariCriAber, anl Epsten, Ila Polosukhin, Jacob Devln, Kenton Lee, KristinaToutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.Natural questions: bencmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452466, 2019. doi: 10.1162/TACL _A _00276. URL https://doi.org/10.1162/tacl_a_00276.   \n[] Juyou L, Qin Zhang, Yangin Yu, QiangFu, and Deheg Y. Moregents  you nee.Trans. Mach. Lear. Res., 2024, 2024. URL https://openreview.net/forum?id=bgzuSZ8aeg.   \n[34] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yuju Yang, Shuming Shi, and Zhaopeng Tu. Encouraging divergent thinking in large language models through multi-agent debate. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 17889 17904. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.EMNLP-MAIN.992. URL https://doi.org/10.18653/v1/2024.emnlp-main.992.   \n[35] Tianqi Liu, Yao Zhao, Risha Joshi, Misha Khalman, Mohamd Saleh, Peter J. Liu, and Jialu LiuStatisical rejection sampling improves preference optimization. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openrevi ew.net/forum?id=xbjSwwrQOe.   \n[36] Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and Silvio Savarese. BOLAA: benchmarking and orchestrating llm-augmented autonomous agents. CoRR, abs/2308.05960, 2023. doi: 10.48550/ARXIV.2308.05960. URL https://doi.org/10.48550/arXiv.2308.05960.   \n[37] Michael Luo, Siun Tan, Justin Won, Xiaoxian Shi, WillamY. Tang, Manan Roonga, Coln Cai, Jeffy Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with a 1.5b model by scaling rl. https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-01-Preview-with-a -1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2, 2025. Notion Blog.   \n[38 AlexMaln, karisai, Victor Zhong Rajarsh Das, HanneHajishz and Daniel KhashabiWhenorust language models: Investigating effectiveness and limitations of parametric and non-parametric memories.CoRR, abs/2212.10511, 2022. doi: 10.48550/ARXIV.2212.10511. URL https://doi.org/10.48550/arXiv.2212.10511.   \n[39] Moonshot AI. Kimi k2: Open agentic intelligence. https://moonshotai.github.io/Kimi-K2/, 2025. URL https://moonshotai.github.io/Kimi-K2/. Accessed: 2025-07-15.   \n[40] Siddharth Nayak, Adelmo Morrison Orozco, Marina Ten Have, Jackson Zhang, Vittal Thirumalai, Darren Chen, Aditya Kapoor, Eric Robinson, Karthik Gopalakrishnan, James Harrison, Anuj Mahajan, Brian Ichter, and Hamsa Balakrishnan. Long-horizon planning for multi-agent robots in partially observable environments. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/7d6e85e88495104442af94c98e899659-Abstrac t-Conference.html.   \n[41] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774.   \n[42]OpenAI. Openai 03 and 04-mini system card. https://cdn.openai.com/pdf/221c875-02dc-4789-800b-e7758 f3722c1/o3-and-04-mini-system-card.pdf, 2025. URL https://cdn.openai.com/pdf/221c875-02dc-478 9-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf.   \n[43] Long Ouyang, Jefrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language mdes  follow instrucions with human eeback. In Sanm Koyeo, S. Mohame, A.Agarwal, Daniele Beave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a0 01731-Abstract-Conference.html.   \n[4] Boln Peng,ChuyuanLi, Pengheng He, Michel Galy, and Jianen Gao. Istrucioui with GPT-4.CoRR, abs/2304.03277, 2023. doi: 10.48550/ARXIV.2304.03277. URL https://doi.org/10.48550/arXiv.2304.03277.   \n[45] Archik Prasad, Alexandr Koller, Mareike Hartan, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. Adapt: As-needed decomposition and planning with language models. In Kevin Duh, Helena GómezAdorno, and Steven Bethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 42264252. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-NAACL.264. URL https://doi.org/10.18653/v1/2024.findings-naac1.264.   \n[6 Or Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and narrowing the composiionality apinlanguage models.InHoudaBouamor, Juan Pno, and KalikaBal editors, Find the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 56875711. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.FINDINGS-EMNLP.378. URL https://doi.org/10.18653/v1/2023.findings-emnlp.378.   \n[47] Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Jiadai Sun, Xinyue Yang, Yu Yang, Shuntian Yo, Wei Xu, Jie Tang, and Yuxio Dong. Webr: Training LLM web agents via self-evolvigonlie curriulum reinforcement learning. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=oVKEAFjEqv.   \n[48] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Xuanhe Zhou, Yufei Huang, Chaojun Xiao, Chi Han, Yi R. Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Guoliang Li, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models. ACM Comput. Surv., 57(4):101:1101:40, 2025. doi: 10.1145/3704435. URL https://doi.org/10.1145/3704435.   \n[49] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.it hub.io/blog/qwq-32b/.   \n[0] Raael Rafailov, ArchitSharma, ricMitchel, Crisopher . Maning, Stefanormon, and Chelse Fin. preference optimization: Your language model is secretly a reward model. In Alice Oh, Tristan Nauman, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/a85b4 05ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html.   \n[51] Matthew Renze and Erhan Guven. Self-reflection in LLM agents: Efects on problem-solving perormance.oRR, abs/2405.06682, 2024. doi: 10.48550/ARXIV.2405.06682. URL https://doi.org/10.48550/arXiv.2405.06682.   \n[52] John Schulman, Sergey Levine, Pieter Abbee, Michael I. Jordan, and Philipp Moritz. Trust region policy otimization.In Francis R.Bach and David M. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Llle, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 1889-1897. JMLR.org, 2015. URL http://proceedings.mlr.press/v37/schulman15.html.   \n[53] John Schulman, Flip Wolski, PrafullaDhariwal, AlecRadford, and Oleg Klimov. Proximal policyoptimiation algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347.   \n[54] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL https://doi.org/10.48550/arXiv.2402.03300.   \n[55] Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar, and Aviral Kumar. Thinking vs. doing: Agents that reason by scaling test-time interaction. CoRR, abs/2506.07976, 2025. doi: 10.48550/ARXIV.2506.07976. URL https://doi.org/10.48550 /arXiv.2506.07976.   \n[56] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridfow: A flexible and effcient RLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam, The Netherlands, 30 March 2025 - 3 April 2025, pages 12791297. ACM, 2025. doi: 10.1145/3689031.3696075. URL https://doi.org/10.1145/3689031.3696075.   \n[57] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36:Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-A bstract-Conference.html.   \n[58] Moit Shridhar, Xingdi Yuan, Marc-Alexandre Cté, Yonatan Bisk, Adam Trischler, and Matthew J. Hausnecht. Alfworld: Aligning text and embodied environments for interactive learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=OIOxOYcCdTn.   \n[59] Charne, Jaehn Lee, Kelvin Xu, an AviralKumarScal LLM test-timecmpuptimy can ee effective than scaling model parameters. CoRR, abs/2408.03314, 2024. doi: 10.48550/ARXIV.2408.03314. URL https://doi.org/10.48550/arXiv.2408.03314.   \n[60] GiuloStarace, Oliver Jafe, Dane Sherbur, Jmes Aung, Jun Shern Chan, LeonMaksin, Rache Dias, Evan Mys, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, and Tejal Patwardhan. Paperbench: Evaluating ai's ability to replicate AI research. CoRR, abs/2504.01848, 2025. doi: 10.48550/ARXIV.2504.01848. URL https://doi.org/10.48550/arXiv.2504.01848.   \n[61] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from feedback with language models. In Alice O, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/b5c8c1c117618267944b2617add0a766-Abstrac t-Conference.html.   \n[62] Richard S. Sutton and Andrew G. Barto. Reinforcement learning - an introduction, 2nd Edition. MIT Press, 2018. URL http://www.incompleteideas.net/book/the-book-2nd.html.   \n[63] Richard S. Sutton, David A. McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinorcement learning with function approximation. In Sara A. Solla, Todd K. Leen, and Klaus-Robert Müller, editors, Advances in Neural Information Processing Systems 12, [NIPS Conference, Denver, Colorado, USA, November 29 - December 4, 1999], pages 10571063. The MIT Press, 1999. URL http://papers.nips.cc/paper /1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.   \n[64] YasharTalebirad andAmihoseinNadiriMuli-agent collaboration: Harnessing the power inteligent LLM agents. CoRR, abs/2306.03314, 2023. doi: 10.48550/ARXIV.2306.03314. URL https://doi.org/10.48550/arX iv.2306.03314.   \n[] GemTeam, Rohnnil SebastBore, Jean-Baptilayrac, JahuYu,Radu Soricut, JohaScha, Andrew M Dai, Anja Hauth, Katie Mllican,  al Gemini: a family of highly capable multimodal models.arXiv preprint arXiv:2312.11805, 2023.   \n[66] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, HuaYuan, JiaChen, Jianhang Guo, Jianl u, Jianzu Wang Jie Zhao, Jin Zhang Jingyuan Liu, JunYan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with llms. CoRR, abs/2501.12599, 2025. doi: 10.48550/ARXIV.2501.12599. URL https://doi.org/10.48550/arXiv.2501.12599.   \n[6] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Trans. Assoc. Comput. Linguistics, 10:539554, 2022. doi: 10.1162/TACL_ A\\_00475. URL https://doi.org/10.1162/tacl_a_00475.   \n[68] Luong Quoc Trung, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 76017614. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.410. URL https://doi.org/10.18653/v1/2024.acl-1ong.410.   \n[69] Ruoyao Wang, Peter A. Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 1127911298. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.EMNLP-MAIN.775. URL https://doi.org/10.18653/v1/2022.emnlp-main.775.   \n[70] Xuezhi Wang, Jason Wei, Dale Schurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw.   \n[71] Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, and Manling Li. RAGEN: understanding self-evolution in LLM agents via multi-turn reinforcement learning. CoRR, abs/2504.20073, 2025. doi: 10.48550/ARXIV.2504.20073. URL https: //doi.org/10.48550/arXiv.2504.20073.   \n[72] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: A simple yet challenging benchmark for browsing agents. CoRR, abs/2504.12516, 2025. doi: 10.48550/ARXIV.2504.12516. URL https: //doi.org/10.48550/arXiv.2504.12516.   \n[73] Ronald J. Williams.Simple statistical gradient-followingalgorithms or connectionistreinforcement earng. Mach. Learn., 8:229256, 1992. doi: 10.1007/BF00992696. URL https://doi.org/10.1007/BF00992696.   \n[74 n Wu, Gaga Bansal, Jey Zhang Yran Wu,Shao ZhangErkang Zhu, Beibni, Li Jiang, Xiu Zhang, and Chi Wang. Autogen: Enabling next-gen LLM applications via multi-agent conversation framework. CoRR, abs/2308.08155, 2023. doi: 10.48550/ARXIV.2308.08155. URL https://doi.org/10.48550/arXiv.2308. 08155.   \n[75] xAI. Grok 4. https://x.ai/news/grok-4, 2025.   \n[76] Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, Honglin Guo, Wei Shen, Xiaoran Fan, Yuhao Zhou, Shihan Dou, Xiao Wang, Xinbo Zhang, Peng Sun, Ta Gu Qi Zhang, nd Xuanjg HuanTrai rgeang models r reasig hro reve currclum reinforcement learning. In Forty-frst International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=t82Y3fmRtk.   \n[77] Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, Songyang Gao, Lu Chen, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang Jiang. Agentgym: Evolving large language model-based agents across diverse environments. CoRR, abs/2406.04151, 2024. doi: 10.48550/ARXIV.2406.04151. URL https://doi.org/10.48550/arXiv.2406.04151.   \n[78] Zhiheng Xi, Dingwen Yang, Jixuan Huang, Jiafu Tang, Guanyu Li, Yiwen Ding, Wei He, Boyang Hong, Shihan Dou, Wenyu Zhan, Xiao Wang, Rui Zheng, Tao Ji, Xiaowei Shi, Yitao Zhai, Rongxiang Weng, Jingang Wang, Xunliang Cai, Tao Gui, Zuxuan Wu, Qi Zhang, Xipeng Qiu, Xuanjing Huang, and Yu-Gang Jiang. Enhancing LLM reasoning via critique models with test-time and training-time supervision. CoRR, abs/2411.16579, 2024. doi: 10.48550/ARXIV.2411.16579. URL https://doi.org/10.48550/arXiv.2411.16579.   \n[79] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senje Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, Qi Zhang, and Tao Gui. The rise and potential of large language model based agents: a survey. Sci. China Inf. Sci., 68(2), 2025. doi: 10.1007/S11432-024-4222-0. URL https://doi.org/10.1007/s11432-024-4222-0.   \n[80] Han Xia, Songyang Gao, Qiming Ge, Zhiheng Xi, Qi Zhang, and Xuanjing Huang. Inverse-q\\*: Token level reinforcement learning for aligning large language models without preference data. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics:EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 81788188. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-EMNLP.478. URL https://doi.org/10.18653/v1/2024.findings-emnlp.4 78.   \n[81] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Qizhe Xie. Self-evaluation guided beam search for reasoning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/81fde95c4dc79188a69ce5b 24d63010b-Abstract-Conference.html.   \n[82] Zhihui Xie, Jie Chen, Liyu Chen, Weichao Mao, Jingjing Xu, and Lingpeng Kong. Teaching language models to critique via reinforcement learning. CoRR, abs/2502.03492, 2025. doi: 10.48550/ARXIV.2502.03492. URL https://doi.org/10.48550/arXiv.2502.03492.   \n[83] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report.CoRR, abs/2412.15115, 2024. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412.15115.   \n[84] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025. doi: 10.48550/ARXIV.2505.09388. URL https://doi.org/10.48550/arXiv.2505.09388.   \n[85] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Elen Rilof David Chiang, Julia Hockenmaier, and Jun'ichi Tsuj, editors, Procedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 23692380. Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-1259. URL https://doi.org/10.18653/v1/d18-1259.   \n[86] Shunyu Yao. Language Agents: From Next-Token Prediction to Digital Automation. PhD thesis, Princeton University, 2024.   \n[87] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/82ad13ec01f9fe44c01cb91814f d7b8c-Abstract-Conference.html.   \n[88] Shunyu Yao, Jefrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview .net/forum?id=WE_vluYUL-X.   \n[9] Junje Ye, Yilong Wu, Sixian Li, Yumng Yang, Tao Gui, Qi Zhang, Xuang Huang,Peng Wang, Zhongcho Shi, Jnng an, nd Zhengy u T-rai: task-featubasamork r rain re lngugeol in tool use. CoRR, abs/2412.15495, 2024. doi: 10.48550/ARXIV.2412.15495. URL https://doi.org/10.48550 /arXiv.2412.15495.   \n[90] Junje Ye, Zhengyin Du, Xuesong Yao, Weijan Lin, Yufei Xu, Zehui Chen, Zaiyuan Wang, Sining Zhu, Zhiheng Xi, Siyu Yuan, Tao Gui, Qi Zhang, Xuanjing Huang, and Jiecao Chen. Toolhop: A query-driven benchmark for evaluating large language models in multi-hop tool use. CoRR, abs/2501.02506, 2025. doi: 10.48550/ARXIV.250 1.02506. URL https://doi.org/10.48550/arXiv.2501.02506.   \n[91] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxio Dong, and Jie Tang. Agenttuni: Enablng generalized agent abilitis for ms. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings o the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 30533077. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-A CL.181. URL https://doi.org/10.18653/v1/2024.findings-acl.181.   \n[92]J Zhan Tian Lan RihMurty, Zhiu WYo, JunTan, Thai Hag, LYag Y Feng, Zuxin Liu, Tulika Manoj Awalgaonkar, Juan Carlos Niebles, Silvio Savarese, Shelby Heinecke, Huan Wang, and Caiming Xiong. Agentohana: Design unified data and training pipeline for effective agent learning. CoRR, abs/2402.15506, 2024. doi: 10.48550/ARXIV.2402.15506. URL https://doi.org/10.48550/arXiv.2402.15506.   \n[93] Rui Zheng, Shihan Dou, Songyng Gao, Yuan Hua, Wei Shen, Binai Wang, Yan Liu, Snje Jin, Qin Liu, Yuh Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang, Zhangyue Yin, Rongxiang Weng, Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, and Xuanjing Huang. Secrets of RLHF in large language models part I: PPO. CoRR, abs/2307.04964, 2023.doi: 10.48550/ARXIV.2307.04964. URL https://doi.org/10.48550/arXiv.2307.04964.   \n[94] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=oKn9c6ytLx.   \n[95 ZhouAnd Zanee, Jyi an ery Lvie,anviralrcheTains vhierarchical multi-turn RL In Forty-first Interational Conference on Machine Learning, CML 024, Viena, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=b6rAOkAHT1.   \n[96] King Zhu, Hanhao Li, Siwei Wu, Tianshun Xing, Dehua Ma, Xiangru Tang, Minghao Liu, Jian Yang, Jiaheng Liu, Yuchen Eleanor Jiang, Changwang Zhang, Chenghua Lin, Jun Wang, Ge Zhang, and Wangchunshu Zhou. Scaling test-time compute for LLM agents. CoRR, abs/2506.12928, 2025. doi: 10.48550/ARXIV.2506.12928. URL https://doi.org/10.48550/arXiv.2506.12928.   \n[97] Tinghui Zhu, Kai Zhang, Jian Xie, and Yu Su. Deductive beam search: Decoding deducible rationale for chain-of-thought reasoning. CoRR, abs/2401.17686, 2024. doi: 10.48550/ARXIV.2401.17686. URL https: //doi.org/10.48550/arXiv.2401.17686. ",
        "bbox": [
            106,
            111,
            890,
            906
        ],
        "page_idx": 22
    },
    {
        "type": "discarded",
        "text": "23 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            109,
            58,
            892,
            911
        ],
        "page_idx": 23
    },
    {
        "type": "discarded",
        "text": "24 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            107,
            80,
            897,
            912
        ],
        "page_idx": 24
    },
    {
        "type": "discarded",
        "text": "25 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            107,
            63,
            892,
            896
        ],
        "page_idx": 25
    },
    {
        "type": "discarded",
        "text": "26 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            107,
            64,
            890,
            895
        ],
        "page_idx": 26
    },
    {
        "type": "discarded",
        "text": "27 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            106,
            40,
            892,
            910
        ],
        "page_idx": 27
    },
    {
        "type": "discarded",
        "text": "28 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            106,
            61,
            890,
            901
        ],
        "page_idx": 28
    },
    {
        "type": "discarded",
        "text": "29 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            106,
            98,
            888,
            315
        ],
        "page_idx": 29
    },
    {
        "type": "discarded",
        "text": "30 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Appendix ",
        "text_level": 1,
        "bbox": [
            114,
            94,
            246,
            118
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "A Details of the AgentGym-RL Architecture ",
        "text_level": 1,
        "bbox": [
            114,
            135,
            549,
            152
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "This appendix provides a detailed description of the AgentGym-RL architecture, complementing the highlevel overview in the main text (Figure 2). We present the implementation details of the three core modules—Environment, Agent, and Training. These details highlight the engineering considerations that ensure scalability, flexibility, and reproducibility in large-scale RL experiments. ",
        "bbox": [
            114,
            165,
            883,
            226
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Environment module. In this module, each environment is packaged as an independent service with the option of deploying multiple replicas to support parall requests. An environment client communicates with the environment server via HTTP and exposes APIs to the agent, including /observation to get the current observation from the environment, /available_actions to get the curently available actions, /step to perform an action, and /reset to reset the environment. Currently, AgentGym-RL covers five major scenario categories. This modular server-client design allows new environments to provide comprehensive environment and data support for LLM agent training. ",
        "bbox": [
            112,
            242,
            883,
            349
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Agent module. The agent module encapsulates the reasoningaction loop of LLM-based agents. It receives obervations from the environment, performs reasoning over multiple turns, and outputs actions (e.g invokig provided APIs). The module supports different prompting strategies, sampling configurations, and reward functions. ",
        "bbox": [
            112,
            366,
            883,
            426
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Training module. The training module provides a unified reinforcement learning (RL) pipeline that supports both online and offine algorithms, offering researchers a flexible foundation for large-scale LLM agent training. The module manages the entire RL lifecycle: trajectory collection, advantage estimation, policy optimization, and reward shaping, while also supporting curriculum learning and staged interaction scaling (i.e., ScalingInter-RL). ",
        "bbox": [
            112,
            443,
            883,
            520
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "The entire training pipeline can be distributed across multiple nodes, leveraging both multi-process and multi-node parallelism. Effcient batching and asynchronous logging utilities ensure that system throughput scales with additional compute resources. Diagnostics tools are integrated to provide fine-grained metrics, including poliyentropy, KL divergence, reward curves, and rollout statistics, which are automaticaly recorded for later analysis and reproducibility. ",
        "bbox": [
            112,
            526,
            883,
            603
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "B Implementation Details and Settings of Each Environment ",
        "text_level": 1,
        "bbox": [
            117,
            617,
            709,
            636
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "We conduct allthe experiments on NVIDIA A100 GPUs and Ascend 910B NPUs. The remaining part of this section shows detailed setting of different environments. ",
        "bbox": [
            112,
            647,
            883,
            678
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "B.1 Web Navigation Scenario ",
        "text_level": 1,
        "bbox": [
            114,
            691,
            385,
            708
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Tools and APIs. In web navigation scenario, the agent simulates human interaction with web pages to ultimately complete the task. WebArena[94] supports these interactioins through a set of tool APIs, allowi agents to perform  variety  real-word tasks,including nlneshoppingengaging in discussionsn Reddit, collaborating on software development via GitLab, and managing store content through a CMS. In addition to these online platforms, WebArena also provides three utility-style tools:a map for navigation and location-based information search, a calculator, and a scratchpad for note-taking. ",
        "bbox": [
            112,
            717,
            883,
            808
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "A query case of web navigation is shown below: ",
        "bbox": [
            112,
            814,
            454,
            830
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Web Navigation Example ",
        "text_level": 1,
        "bbox": [
            129,
            837,
            869,
            861
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "You are an autonomous intelligent agent tasked with navigating a web browser. You will be given web-based tasks. These tasks will be accomplished through the use of specific actions you can issue. ",
        "bbox": [
            135,
            867,
            861,
            898
        ],
        "page_idx": 30
    },
    {
        "type": "discarded",
        "text": "31 ",
        "bbox": [
            490,
            938,
            506,
            949
        ],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Available Information: ",
        "text_level": 1,
        "bbox": [
            138,
            108,
            299,
            121
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "•User's objective: The task to complete   \nAccessibility tree: Simplified webpage representation, providing key information. •Current URL: The active page's address   \nOpen tabs: Currently available tabs   \nPrevious action: Last performed action ",
        "bbox": [
            163,
            122,
            758,
            196
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Action Categories: ",
        "text_level": 1,
        "bbox": [
            138,
            207,
            272,
            219
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Page Operations: ",
        "bbox": [
            140,
            222,
            263,
            234
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "• click [id]: Click element with ID   \ntype [id] [content] [0|1]: Input text (1=press Enter)   \nhover [id]: Hover over element   \npress [key_comb]: Simulate key press (e.g., Ctrl+v)   \nscroll [down|up]: Scroll page direction ",
        "bbox": [
            163,
            236,
            599,
            310
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Tab Management: ",
        "bbox": [
            138,
            311,
            266,
            324
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "new_tab: Open new tab • tab_focus [tab_index]: Switch to tab •close_tab: Close current tab ",
        "bbox": [
            163,
            327,
            470,
            369
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "URL Navigation: ",
        "bbox": [
            138,
            372,
            259,
            385
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "•goto [url]: Navigate to URL go_back: Return to previous page go_forward: Advance to next page ",
        "bbox": [
            163,
            387,
            434,
            431
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Completion: ",
        "bbox": [
            138,
            433,
            225,
            445
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "stop [answer]: Submit final answer (or \"N/A\" if you believe the task is impossible to complete) ",
        "bbox": [
            148,
            446,
            862,
            462
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Homepage: If you want to visit other websites, check out the homepage at http://homepage. com. ",
        "bbox": [
            138,
            469,
            844,
            484
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Objective: Among the top 10 post in \"books\" forum, show me the book names from posts that recommand a single book. ",
        "bbox": [
            135,
            501,
            866,
            532
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Settings. We include five subtasks: E-commence, Reddit, Gitlab, OpenStreetMap (Map), and online store content management system (CMS), comprising a total of 372 training queries and 50 testing queries. These are selected from the origin WebArena dataset, which contains 812 queries across three categories:Information Seeking, Site Navigation, and Content & Cong.To facilitate eficient parall rollout, we exclude the Content & Confg tasks, which involve insert, update and delete operations that change the state of the websites. We set the maximum number of agent-environment interactions to 15 turns. For the SFT baselines, we set the learning rate to $1 \\times 1 0 ^ { - 4 }$ . We employ GRPO as the main RL algorithm with a learning rate of $5 \\times 1 0 ^ { - 7 }$ and a KL cofficient of $1 \\times 1 0 ^ { - 3 }$ . For each query, we sample 4 distinct trajectories using a temperature of 1.0. ",
        "bbox": [
            112,
            559,
            885,
            681
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "B.2 Deep Search Scenario ",
        "text_level": 1,
        "bbox": [
            116,
            694,
            357,
            710
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Tools and APIs. The deep search senario features a search engine-based environment equipped with specialized tools and APIs supporting the interaction with search engines. These APIs enable agents to dynamically generate search queries during the reasoning process, retrieve relevant information from external sourcs, andincorporate the retrieved information into subsequent reasoning steps. This setting lows agents to engage in complex reasoning proceses that involve iterativesearching an information integration, therey enhancing their capability to solve intricate problems where external knowledge is essential. ",
        "bbox": [
            112,
            718,
            883,
            810
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "A query case of Deep Search is shown below: ",
        "bbox": [
            114,
            816,
            434,
            832
        ],
        "page_idx": 31
    },
    {
        "type": "discarded",
        "text": "32 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Deep Search Example ",
        "text_level": 1,
        "bbox": [
            130,
            97,
            875,
            119
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "You must always reason inside $<$ thik $\\supset$ ...</think $>$ first; if you lack knowledge, issue a <search $\\mathcal { > }$ .….</search> and then stop; do not generate $<$ information $>$ or $<$ answer $>$ yet; wait for external input between <information $>$ .….</information $>$ before continuing; resume only when new <information> is given; do not skip steps or anticipate answers early. ",
        "bbox": [
            137,
            127,
            866,
            189
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "Question: Who got the first Nobel Prize in Physics? ",
        "bbox": [
            137,
            205,
            514,
            222
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "Settings. We include queries from 7 datasets following the setup of Search-R1 [26]: NQ [32], TriviaQA [28], PopQA [38], HotpotQA [85], 2wiki [19], Musique [67], and Bamboogle [46]. To ensure fair comparison and balanced evaluation, we randomly sample 400 examples from the development sets of NQ, TriviaQA, PopQA, HotpotQA, 2wiki, Musique, and Bamboogle. The maximum number of agent-environment interactions is set to 4 turns. For the SFT baselines, the learning rate is set to $1 \\times 1 0 ^ { - 4 }$ . We employ GPRO as the main algorithm for reinforcement learning setups with a learning rate of $1 \\times 1 0 ^ { - 6 }$ , a KL cofficient of $1 \\times 1 0 ^ { - 3 }$ , and a sampling temperature of 1.0. We sample 8 distinct trajectories for a single query. ",
        "bbox": [
            112,
            248,
            885,
            354
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "B.3 Digital Games Scenario ",
        "text_level": 1,
        "bbox": [
            114,
            367,
            369,
            385
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "Environments, Tools and APIs. As for digital games, we introduce TextCraft[45], a text-based game environment mirroring Minecraft. The APIs in TextCraft include crafting, inventory management, and dynamic narrative generation. These APIs allow agents to execute predefined crafting recipes, manipulate inventory contents, navigate virtual spaces, dynamically generate quests and sub-tasks based on natural language objectives, and recursively decompose complex tasks into achievable sub-goals. ",
        "bbox": [
            112,
            392,
            883,
            468
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "A query case of TextCraft can be seen below: ",
        "bbox": [
            112,
            476,
            439,
            491
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "TextCraft Example ",
        "text_level": 1,
        "bbox": [
            138,
            502,
            279,
            517
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "You are given few useful crafting recipes to craft items in Minecraft. Crafting commands are of the format \"craft [target object] using [input ingredients]\". ",
        "bbox": [
            133,
            529,
            861,
            559
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "Every round I will give you an observation, you have to respond an action based on the state and instruction. You can \"get\" an object (ingredients) from the inventory or the environment, look-up the game inventory by \"inventory\", or \"craft\" (target) using any of the crafting commands. You can use ONLY these crafting commands provided, do not use your own crafting commands. However, if the crafting command uses a generic ingredient like \"planks\", you can use special types of the same ingredient e.g. \"dark oak planks\" in the command instead. ",
        "bbox": [
            138,
            560,
            862,
            650
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "Goal: Craft flint and steel. ",
        "bbox": [
            137,
            667,
            330,
            683
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "Settings. In TextCraft, task difficulty is measured by the maximum depth of the corresponding crafting tree. In practice, the benchmark contains tasks with crafting trees of depths 1, 2, 3, and 4. Accordingly, we divide the entiretask set into four subsets based n these depths. We set the maximum number o interacions to 20 turns. In the SFT baselines, we set the learning rate to $1 \\times 1 0 ^ { - 4 }$ . We employ GRPO as the main RL algorithm with a learning rate of $1 \\times 1 0 ^ { - 6 }$ , a KL cofficient of $1 \\times 1 0 ^ { - 3 }$ , and a sampling temperature of 1.0. We sample 8 distinct trajectories for a single query. ",
        "bbox": [
            112,
            708,
            885,
            799
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "B.4 Embodied Scenario ",
        "text_level": 1,
        "bbox": [
            114,
            811,
            334,
            829
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "Tools and APIs. We introduce the BabyAI environment as a representative setting for embodied tasks. It provides APIs that allow agents to navigate a controllable grid world using natural language instructions. Through these APIs, agents can perform actions such as moving objects, unlocking doors, and interacting with the environment in response to textual commands. ",
        "bbox": [
            114,
            837,
            885,
            897
        ],
        "page_idx": 32
    },
    {
        "type": "discarded",
        "text": "33 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "A query case of BabyAI can be seen below: ",
        "bbox": [
            114,
            98,
            423,
            113
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "BabyAI Example ",
        "text_level": 1,
        "bbox": [
            120,
            121,
            877,
            145
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "You are an exploration master that wants to finish every goal you are given. Every round I will give you an observation, and you have to respond an action and your thought based on the observation to finish the given task. You are placed in a room and you need to accomplish the given goal with actions. You can use the following actions: ",
        "bbox": [
            137,
            151,
            864,
            212
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "- turn right - turn left - move forward - go to obj id - pick up obj id   \n- go through door id: door must be an open door.   \n- toggle and go through door id: door can be a closed door or a locked door. If you want to open a locked door, you need to carry a key that is of the same color as the locked door.   \n- toggle: there is a closed or locked door right in front of you and you can toggle it. ",
        "bbox": [
            135,
            212,
            862,
            289
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "Your goal: Go to the red ball. ",
        "bbox": [
            138,
            305,
            354,
            320
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "Settings. Following the original implementation, we divide the tasks into six subsets based on the final goal. We set the maximum number of interactions to 20 turns. In SFT baselines, we set the learning rate to $1 \\times 1 0 ^ { - 4 }$ . We employ GRPO as the main RL algorithm with a learning rate of $1 \\times 1 0 ^ { - 6 }$ , a KL cofficient of $1 \\times 1 0 ^ { - 3 }$ , and a sampling temperature of 1.0. We sample 8 distinct trajectories for a single query. ",
        "bbox": [
            112,
            348,
            885,
            409
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "B.5 Scientific Scenario ",
        "text_level": 1,
        "bbox": [
            114,
            421,
            326,
            439
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "Tools and APIs. SciWorld[69] is an agent environment for scientific tasks. It provides APIs that are designed to support scientific exploration through text-driven reasoning cycles. These APIs empower agents to conduct experiments by interacting with various scientific apparatus and performing actions like measuring temperature, connecting electrical circuits, and mixing chemicals. ",
        "bbox": [
            112,
            446,
            883,
            508
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "A query case of SciWorld can be seen below: ",
        "bbox": [
            114,
            515,
            433,
            530
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "SciWorld Example ",
        "text_level": 1,
        "bbox": [
            137,
            541,
            272,
            556
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "You are an agent for science world. Every round I will give you an observation, you have to respond an action based on the observation to finish the given task. ",
        "bbox": [
            132,
            568,
            866,
            599
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "Your task is to boil water. For compounds without a boiling point, combusting the substance is also aptable.Firs, focus on the substance.Then, take actions that will cause t to change ts stat matter. ",
        "bbox": [
            137,
            616,
            862,
            661
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "Settings. We select 8 subsets of tasks from the original SciWorld environment. We set the maximum number of interactions between the agent and the environment to 20 turns. In SFT baselines, we set the learning rate to $1 \\times 1 0 ^ { - 4 }$ . We employ GRPO as the main RL algorithm with a learning rate of $1 \\times 1 0 ^ { - 6 }$ , a KL cofficient of $1 \\times 1 0 ^ { - 3 }$ , ans p peature  We mpe  isti ajr or  seqy. ",
        "bbox": [
            112,
            685,
            885,
            747
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "C Trajectory examples and visualizations of our RL agent ",
        "text_level": 1,
        "bbox": [
            111,
            762,
            691,
            781
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "This appendix provide aditionalrajectoryvisualizations acros multiple evionents.The guresillustrate the behaviors of both baseline and RL-trained agents, highlighting the RL model's superior performance in exploration, task execution, and interaction patterns, while also revealing common failure modes that remain. ",
        "bbox": [
            112,
            792,
            887,
            839
        ],
        "page_idx": 33
    },
    {
        "type": "discarded",
        "text": "34 ",
        "bbox": [
            488,
            938,
            508,
            949
        ],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "BabyAI ",
        "text_level": 1,
        "bbox": [
            460,
            175,
            534,
            194
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Task: go to the blue box   \nObs 0: In front of you in this room, you can see several objects: There is a yellow closed door 12 ses in front of you and 1 steps to your ght.There is a green closed door 1 3steps in front of you and 3 steps to your left.The room has walls around you. You are facing a wall 3 steps away. You are not carrying anything.   \nAvailable actions: \"urn left\", \"turn right\", \"move forward\", \"toggle and go throug yellow closed door ${ \\mathfrak { T } } \" .$ , \"toggle and go through green closed door ${ \\mathfrak { T } } \"$ , \"go to yellow closed door $1 \"$ , \"go to green closed door ${ \\mathfrak { T } } ^ { \\mathfrak { n } } ,$ \"check available actions\"] ",
        "bbox": [
            137,
            202,
            700,
            301
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Base Model ",
        "text_level": 1,
        "bbox": [
            246,
            319,
            354,
            334
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Our RL Model ",
        "text_level": 1,
        "bbox": [
            630,
            319,
            761,
            334
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Thought 1: To reach the blue box, I need to move forward, but there is a yellow closed door in front of me, blocking the way. …it's more efficient to go through the yellow closed door first.   \nAct 1: toggle and go through yellow closed door 1   \nObs 1: There is a green closed door 1 6 steps in front of you and 2 steps to your right. The room has walls around you. You are facing a wall 6 steps away. ",
        "bbox": [
            130,
            344,
            462,
            417
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Thought 2: …. Now that I have gone through the yellow closed door, Since I don't have any keys, I need to toggle and go through this door to find the blue box. Act 2: toggle and go through green closed door 1 Obs 2: There is a grey closed door 1 4 steps in front of you and 3 steps to your left. The room has walls around you. You are facing a wall 6 steps away. ",
        "bbox": [
            130,
            425,
            455,
            488
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Thought 3: I now face a grey closed door directly in front of me. I need to toggle and go through the grey closed door to continue my journey to find the blue box. Act 3: toggle and go through grey closed door 1 Obs 3: There is a red closed door 1 6 steps in front of you and 3 steps to your left. The room has walls around you. You are facing a wall 6 steps away. ",
        "bbox": [
            130,
            496,
            467,
            559
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Thought 4: .my only option is to toggle it and attempt to go through.   \nAct 4: toggle and go through red closed door 1   \nObs 4: The room has walls around you. You are facing a wall 6 steps away.   \nAct 5: move forward   \nAct 6: go through red open door 1   \nAct 7: go through grey open door 1   \nAct 13: go through grey open door 1   \nAct 14: go through red open door 1 ",
        "bbox": [
            129,
            568,
            457,
            666
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Thought 15: Given that the room has walls around me and I am not carrying anything, it appears I have reached the end of this path. I have already explored this area thoroughly and there are no more objects or doors to interact with. Act 15: check available actions ",
        "bbox": [
            132,
            674,
            450,
            719
        ],
        "page_idx": 34
    },
    {
        "type": "image",
        "img_path": "images/9c0b745102129fed80d3bf129a4872a01dd3e826d43d6f740ee5b7c5b4be5653.jpg",
        "image_caption": [
            "Figure 12 Smart case of our RL agent compared to the base agent on BabyAI task. "
        ],
        "image_footnote": [],
        "bbox": [
            712,
            188,
            856,
            299
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Thought 20: Turning left again has brought me to a wall, confirming that I have explored all possible directions in this room. ",
        "bbox": [
            132,
            728,
            467,
            755
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Act 20: check available actions ",
        "bbox": [
            132,
            756,
            299,
            765
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Thought 1: .. find the blue box Act 1: toggle and go through yellow closed door 1 Obs 1: There is a green closed door 1 6 steps in front of you and 2 steps to your right. The room has walls around you. You are facing a wall 6 steps away. ",
        "bbox": [
            526,
            344,
            861,
            398
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Thought 2: .. find the blue box   \nAct 2: go to green closed door 1   \nObs 2: There is a green closed door 1 right in front of you 1 steps away. The room has walls around you. ",
        "bbox": [
            526,
            409,
            854,
            453
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Thought 3: .. find the blue box Act 3: toggle and go through green closed door 1 Obs 3: There is a grey closed door 1 4 steps in front of you and 3 steps to your left. The room has walls around you. You are facing a wall 6 steps away. ",
        "bbox": [
            526,
            462,
            861,
            517
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Act 4: go to grey closed door 1   \nAct 5: toggle and go through grey closed door 1   \nAct 6: go to red closed door 1   \nAct 7: turn right   \nAct 8: turn right   \nAct 9: go through grey open door 1   \nAct 10: turn right   \nAct 11: go through green open door 1   \nAct 12: go through yellow open door 1 ",
        "bbox": [
            526,
            534,
            825,
            635
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Act 13:toggle and go through green closed door 1 Obs 13: There is a blue box 1 4 stepsin front of you and 1 steps to your left. There is a green closed door 1 6 steps in front of you and 3 steps to your right. The room has walls around you. You are facing a wall 6 steps away. ",
        "bbox": [
            526,
            645,
            862,
            709
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Act 14:go to blue box ",
        "bbox": [
            527,
            719,
            669,
            729
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Score: 0.0 ",
        "text_level": 1,
        "bbox": [
            251,
            801,
            351,
            814
        ],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "Score: 1.0 ",
        "text_level": 1,
        "bbox": [
            642,
            803,
            751,
            818
        ],
        "page_idx": 34
    },
    {
        "type": "discarded",
        "text": "35 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 34
    },
    {
        "type": "image",
        "img_path": "images/53529f8fab091b65614a89133a6149d2b25d289c398288df1efeae0b3e53b64d.jpg",
        "image_caption": [
            "Figure 13 Trajectory visualization of our RL agent on BabyAI task. "
        ],
        "image_footnote": [],
        "bbox": [
            165,
            253,
            841,
            724
        ],
        "page_idx": 35
    },
    {
        "type": "discarded",
        "text": "36 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "SciWorld ",
        "text_level": 1,
        "bbox": [
            450,
            174,
            547,
            193
        ],
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "Task: Find a(n) living thing. First, focus on the thing. Then, move it to the purple box in the bedroom.   \nhive dorisclosed. a flower pot 3 (containing nothing), a flower pot 4 containing a peach tree in thedult sta,sil,a ower pot (ntaiiga p plant  the dult tage with talheight,sol,a owe 8 (containing nothing), a flower pot 9 (containinga banana tree in theadult stage, soil), a jug (containing a is closed) A door to the outside (that is closed) ",
        "bbox": [
            129,
            209,
            848,
            303
        ],
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "Base Model ",
        "text_level": 1,
        "bbox": [
            246,
            319,
            354,
            334
        ],
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "Our RL Model ",
        "text_level": 1,
        "bbox": [
            630,
            319,
            761,
            334
        ],
        "page_idx": 36
    },
    {
        "type": "image",
        "img_path": "images/985efbe04668d18a8b0e284faf950df5a1469551781cad1aca292f499cbd2a11.jpg",
        "image_caption": [
            "Figure 14 Smart case of our RL agent compared to the base model on SciWorld task. "
        ],
        "image_footnote": [],
        "bbox": [
            124,
            335,
            875,
            823
        ],
        "page_idx": 36
    },
    {
        "type": "discarded",
        "text": "37 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "SciWorld ",
        "text_level": 1,
        "bbox": [
            648,
            184,
            745,
            200
        ],
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "SciWorld   \nTask: Your task is to measure the melting point of water, which is located around the kitchen. First, focus on the thermometer. Next, focus on the water. If the melting point of water is above -10.0 degrees celsius, focus on the red box. If the melting point of water is below -10.0 degrees celsius, focus on the green box. The boxes are located around the kitchen. Obs 0: This room is called the bedroom. In it, you see: the agent a substance called air a bed. On the bed is: a mattress. On the mattress is: a white   \npillow.. a book shelf (containing A book (The   \nFoundations of the Origin of Species) titled The Foundations of the Origin of Species by Charles   \nDarwin) a closet. The closet door is closed. a   \npainting a table. On the table is: nothing. You also see: A door to the hallway (that is closed)   \nThought 6: Around here, I found a finger painting. Let's go to the kitchen to find a thermometer.   \nAct 6: go to the kitchen   \nObs 6: The door is not open.   \nThought 7: The door to the kitchen is not open. Let's open it before going through.   \nAct 7: open the door to the kitchen   \nObs 7: The door is now open.   \nThought 8: The door to the kitchen is now open. Let's go through to the kitchen.   \nAct 8: go to the kitchen   \nObs 8: You move to the kitchen.   \n\\*\\*\\*\\*   \nThought 10: Around the kitchen, I found a thermometer and a red box. Let's pick up the thermometer and add it to my inventory.   \nAct 10: pick up the thermometer   \nObs 10: You move the thermometer to the inventory. Thought 11: I picked up the thermometer and added it to my inventory. Let's focus on the thermometer. Act 11: focus on the thermometer   \nObs 11: You focus on the thermometer.   \nThought 12: I focused on the thermometer. Let's focus on the water to measure the melting point.   \nAct 12:focus on the water   \nObs 12: No known action matches that input.   \nThought 13: I focused on the thermometer. Let's go to the sink to focus on the water and measure the   \nmelting point.   \nAct 13:focus on the water   \nObs 13: No known action matches that input   \nThought 14: The melting point of water is 0 degrees celsius. Since the melting point of water is above - 10.0 degrees celsius, I will focus on the red box. Act 14: focus on the red box   \nObs 14: You focus on the red box.   \nScore: 0.0 Task: Your task is to find the animal with the   \nlongest life span. The animals are in the 'outside' location. Focus on the animal with the longest life span.   \nObs 0: This room is called the hallway. In it, you see: the agent a substance called air a finger   \npainting You also see: A door to the art studio (that is closed) A door to the bedroom (that is closed) A door to the greenhouse (that is closed) A door to the kitchen (that is closed) A door to the living room (that is closed) A door to the workshop (that is closed)   \nAct 6: go to the outside   \nObs 6: The door is not open.   \nThought 7: The door to the outside is not open. Let's open it before going through.   \nAct 7: open the door to the outside   \nObs 7: The door is now open.   \nThought 8: The door to the outside is now open. Let's go through to the outside.   \nAct 8: go to the outside   \nObs 8: You move to the outside.   \nThought 9: I have gone to the outside. Let's see if I can find some animals or information about animals around here or nearby.   \nAct 9: look around   \nObs 9: This outside location is called the outside. Here you see: the agent a substance called air an axe a chameleon egg a fire pit (containing nothing) a fountain (containing a substance called water) the ground a parrot egg a baby rabbit a substance called wood You also see: A door to the foundry (that is closed) A door to the greenhouse (that is open) A door to the kitchen (that is closed)   \nThought 10: Around here, I found a chameleon egg, a parrot egg, and a baby rabbit. Let's find out the life span of these animals.   \nAct 10: pick up the chameleon egg   \nObs 10: You move the chameleon to the inventory. Thought 11: I picked up the chameleon egg and added it to my inventory. Let's focus on the chameleon egg. Act 11:focus on the chameleon egg   \nObs 11: You focus on the chameleon egg.   \nCrona: aa ",
        "bbox": [
            129,
            178,
            468,
            820
        ],
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            524,
            207,
            862,
            803
        ],
        "page_idx": 37
    },
    {
        "type": "discarded",
        "text": "38 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "WebArena ",
        "text_level": 1,
        "bbox": [
            254,
            185,
            346,
            200
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "WebArena ",
        "text_level": 1,
        "bbox": [
            650,
            185,
            741,
            200
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Task: What are the key aspects that the customers don't like about Zing Jump Rope   \nObs 0:[1] RootWebArea 'Dashboard / Magento Admin' focused: True   \n[108] link '\\ue609 MARKETING'   \n[361] link 'Catalog Price Rule'   \nThought 1: The information provided does not seem to contain this data. We can navigate to the 'MARKETING' section to see if there is any customer feedback or survey data that might be relevant.   \nAct 1: click [108]   \nObs 1: [376] link 'Email Templates'   \nThought 2: Given the objective, we should focus on the Communications section, as it might contain   \ncustomer feedback or survey data.   \nAct 2: click [376]   \nObs 2: [1198] RootWebArea 'Email Templates /   \nCommunications / Marketing / Magento Admin' focused: True   \nThought 3: We can navigate to the \"Catalog Price Rule\" or \"Cart Price Rules\" under the \"Promotions\" section to see if there is any related information. Act 3: click [361]   \nObs 3: no matching element found.   \nThought 4: The \"Marketing\" section might have other relevant sections that we have not explored yet. Act 4: click [108]   \nObs 4: [1649] StaticText 'SE0 & Search'   \nThought 5: The \"SEO & Search\" section might have relevant data, such as search terms or URL rewrites that could indicate customer dissatisfaction.   \nAct 5: click [1649]   \nObs 5: [1670] StaticText 'User Content'   \nThought 6: We can navigate to the \"User Content\" section to see if there is any relevant data,   \nAct 6: click [1670]   \nObs 6: [1674] link 'All Reviews'   \nThought 7: The reviews section is a potential source of customer feedback.   \nAct 7: click [1674]   \nObs 7: [2354] link '\\ue609 MARKETING'   \nThought 8: We need to navigate to the reviews for Zing Jump Rope to find the relevant information. Act 8: click [2354]   \nObs 8: [2818] link 'All1 Reviews'   \nAct 9: click [2818]   \nObs 9: [5172] gridcell 'Any' required: False   \n[5496] combobox  hasPopup: menu expanded: False| Act 10: click [5172]   \nAct 11: click [5496]   \nAct 12: type [5496] [Zing Jump Rope] [1]   \nThoght :the data might ot  available o Zing Jump Rope.   \nAct 13: stop [N/A]   \nScore: 0.0 Task: Who gave 4 or 5 stars for phone cases from EYZUTAK   \nObs 0: [1] RootWebArea 'One Stop Market' focused: True [972] menuitem '\\ue622 Ce11 Phones &   \nAccessories' hasPopup: menu   \nThought 1: We need to navigate to a page where we can see the ratings for EYzUTAK's phone cases   \nAct 1: click [972]   \nObs 1: [4955] link 'Cases, Holsters & Sleeves( 457 item )'   \nThought 2: \"Cases, Holsters & Sleeves\" which likely contains the phone cases from EYzUTAK.   \nAct 2: click [4955]   \nObs 2: [4954] link 'Accessories( 1924 item )'   \nThought 3: We need to navigate to a page where we can see the ratings for these phone cases.   \nAct 3: click [4954]   \nObs 3: [6748] combobox '\\ue615 Search' autocomplete: both hasPopup: listbox required: False expanded:   \nFalse   \nThought 4: We need to find the specific section for EYZUTAk phone cases and then check the ratings.   \nAct 4: type [6748] [EYZUTAK phone cases] [1]   \nObs 4: [9380] DescriptionListDetail '' [9489] link 'EYZUTAK phone case' [10300] img 'Image'   \nThought 5: The search results are displayed in a grid format, and the ratings are likely to be found on the product pages.   \nAct 5: click [10300]   \nThought 6: On a product page, the ratings and reviews are typically found in a section dedicated to   \ncustomer reviews.   \nAct 6: hover [10300]   \nThought 7: However, the ratings and reviews are not immediately visible. We need to navigate to a   \nspecific product page to see the customer reviews and ratings.   \nAct 7: click [10300]   \nThought 9: If the customer reviews section is not immediately visible, scrolling down will help to reveal it.   \nAct 9: scroll [down]   \nObs 9: StaticText 'Rating:'[10513] generic '75%'   \nAct 10: type [10513] [4 or 5] [0]   \nObs 10: 10415] LayoutTable   \nThought 11: we need to check each product's reviews to find the relevant information.   \nAct 11: click [10415]   \nThought 15: The ratings and reviews for this specific product are not relevant to the objective. we need to navigate back to the search results page to continue the search.   \nAct 15: tab_focus [0]   \nScore: 0.0 ",
        "bbox": [
            129,
            204,
            464,
            805
        ],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            521,
            202,
            869,
            819
        ],
        "page_idx": 38
    },
    {
        "type": "discarded",
        "text": "39 ",
        "bbox": [
            490,
            938,
            508,
            949
        ],
        "page_idx": 38
    }
]